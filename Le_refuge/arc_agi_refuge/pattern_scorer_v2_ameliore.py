#!/usr/bin/env python3
"""
PatternScorer v2 Am√©lior√© - √âvaluation Anti-Surapprentissage Avanc√©e
M√©triques sophistiqu√©es pour pr√©venir le surapprentissage syst√©mique
"""

import json
from typing import Dict, List, Tuple, Any, Optional
from collections import defaultdict
import math
import statistics
import copy

class PatternScorerAmeliore:
    """
    PatternScorer v2 am√©lior√© avec m√©triques anti-surapprentissage avanc√©es
    """

    def __init__(self):
        # Seuils de d√©tection du surapprentissage
        self.overfitting_threshold = 0.3
        self.generalization_weight = 0.4
        self.simplicity_weight = 0.3
        self.robustness_weight = 0.3

        # M√©triques de validation crois√©e
        self.cross_validation_folds = 3
        self.validation_split_ratio = 0.2

        # Historique des √©valuations pour d√©tecter les tendances
        self.evaluation_history = defaultdict(list)

        # M√©triques de r√©f√©rence pour diff√©rents types de patterns
        self.baseline_metrics = {
            'spatial': {
                'generalization': 0.65,
                'simplicity': 0.7,
                'robustness': 0.75
            },
            'color': {
                'generalization': 0.75,
                'simplicity': 0.8,
                'robustness': 0.85
            },
            'structural': {
                'generalization': 0.6,
                'simplicity': 0.65,
                'robustness': 0.7
            },
            'mathematical': {
                'generalization': 0.7,
                'simplicity': 0.75,
                'robustness': 0.8
            }
        }

    def evaluate_pattern_comprehensive_advanced(self, pattern_name: str,
                                               pattern_data: Dict[str, Any],
                                               input_grid: List[List[int]],
                                               output_grid: List[List[int]],
                                               training_examples: Optional[List[Dict[str, Any]]] = None,
                                               validation_examples: Optional[List[Dict[str, Any]]] = None) -> Dict[str, Any]:
        """
        √âvaluation compl√®te et avanc√©e d'un pattern avec m√©triques anti-surapprentissage
        """
        evaluation = {
            'pattern_name': pattern_name,
            'timestamp': self._get_timestamp(),
            'metrics': {},
            'overfitting_analysis': {},
            'validation_results': {},
            'recommendations': [],
            'warnings': [],
            'improvement_suggestions': []
        }

        # M√©triques de base
        evaluation['metrics'] = self._compute_advanced_metrics(
            pattern_name, pattern_data, training_examples, validation_examples
        )

        # Analyse du surapprentissage
        evaluation['overfitting_analysis'] = self._analyze_overfitting_risk_advanced(
            evaluation['metrics'], training_examples
        )

        # Validation crois√©e
        if training_examples and len(training_examples) >= self.cross_validation_folds:
            evaluation['validation_results'] = self._perform_cross_validation(
                pattern_name, pattern_data, training_examples
            )
        else:
            evaluation['validation_results'] = {'status': 'insufficient_data'}

        # Score final avec analyse de tendance
        evaluation['final_score'] = self._compute_final_score_with_trends(evaluation)

        # Recommandations et avertissements (apr√®s calcul du score final)
        evaluation['recommendations'] = self._generate_advanced_recommendations(evaluation)
        evaluation['warnings'] = self._generate_advanced_warnings(evaluation)
        evaluation['improvement_suggestions'] = self._generate_improvement_suggestions(evaluation)

        # Sauvegarder dans l'historique
        self._update_evaluation_history(pattern_name, evaluation)

        return evaluation

    def _compute_advanced_metrics(self, pattern_name: str, pattern_data: Dict[str, Any],
                                training_examples: Optional[List[Dict[str, Any]]] = None,
                                validation_examples: Optional[List[Dict[str, Any]]] = None) -> Dict[str, float]:
        """
        Calcul de m√©triques avanc√©es
        """
        metrics = {}

        # 1. M√©trique de g√©n√©ralisation avanc√©e
        metrics['generalization_score'] = self._compute_generalization_advanced(
            pattern_name, pattern_data, training_examples, validation_examples
        )

        # 2. M√©trique de simplicit√© avanc√©e
        metrics['simplicity_score'] = self._compute_simplicity_advanced(
            pattern_name, pattern_data
        )

        # 3. M√©trique de robustesse avanc√©e
        metrics['robustness_score'] = self._compute_robustness_advanced(
            pattern_name, pattern_data, training_examples
        )

        # 4. M√©triques de stabilit√©
        stability_metrics = self._compute_stability_metrics(
            pattern_name, pattern_data, training_examples
        )
        metrics.update(stability_metrics)

        # 5. M√©triques de performance
        performance_metrics = self._compute_performance_metrics(
            pattern_name, pattern_data, training_examples
        )
        metrics.update(performance_metrics)

        return metrics

    def _compute_generalization_advanced(self, pattern_name: str, pattern_data: Dict[str, Any],
                                       training_examples: Optional[List[Dict[str, Any]]] = None,
                                       validation_examples: Optional[List[Dict[str, Any]]] = None) -> float:
        """
        Calcul avanc√© de la g√©n√©ralisation
        """
        if not training_examples:
            return 0.5

        generalization_score = 0.0
        total_tests = 0

        # Test sur donn√©es de validation si disponibles
        if validation_examples:
            validation_score = self._test_on_validation_set(
                pattern_name, pattern_data, validation_examples
            )
            generalization_score += validation_score * 0.6
            total_tests += 1

        # Validation crois√©e sur donn√©es d'entra√Ænement
        if len(training_examples) >= 2:
            cv_score = self._compute_cross_validation_score(
                pattern_name, pattern_data, training_examples
            )
            generalization_score += cv_score * 0.4
            total_tests += 1

        if total_tests > 0:
            generalization_score /= total_tests

        # P√©nalit√© pour les patterns trop sp√©cifiques
        specificity_penalty = self._compute_specificity_penalty(pattern_name, pattern_data)
        generalization_score *= (1 - specificity_penalty)

        # Bonus pour la coh√©rence historique
        if pattern_name in self.evaluation_history:
            historical_bonus = self._compute_historical_consistency(pattern_name, generalization_score)
            generalization_score += historical_bonus * 0.1

        return max(0, min(generalization_score, 1.0))

    def _compute_simplicity_advanced(self, pattern_name: str, pattern_data: Dict[str, Any]) -> float:
        """
        Calcul avanc√© de la simplicit√©
        """
        simplicity_score = 1.0

        # Analyse de la complexit√© du nom du pattern
        name_complexity = len(pattern_name.split('.')) + len(pattern_name) / 10
        simplicity_score -= name_complexity * 0.05

        # Analyse des d√©tails du pattern
        details = pattern_data.get('details', {})
        if isinstance(details, dict):
            # Plus de d√©tails = plus complexe (g√©n√©ralement)
            detail_count = len(details)
            simplicity_score -= detail_count * 0.02

            # D√©tails complexes indiquent un pattern complexe
            if any('complex' in str(k).lower() or 'advanced' in str(k).lower() for k in details.keys()):
                simplicity_score -= 0.1

        # Score de base du pattern
        base_score = pattern_data.get('score', 0)
        confidence = pattern_data.get('confidence', 0)

        # Un score tr√®s √©lev√© avec confiance √©lev√©e peut indiquer sur-optimisation
        if base_score > 0.9 and confidence > 0.8:
            simplicity_score -= 0.15
        elif base_score < 0.3:
            # Score faible peut indiquer pattern trop complexe
            simplicity_score -= 0.1

        # Bonus pour les patterns fondamentaux
        fundamental_patterns = [
            'symmetry', 'repetition', 'scaling', 'translation', 'filling',
            'color_mapping', 'rotation', 'homothety', 'reflection'
        ]

        pattern_type = pattern_name.split('.')[-1]
        if pattern_type in fundamental_patterns:
            simplicity_score += 0.1

        return max(0, min(simplicity_score, 1.0))

    def _compute_robustness_advanced(self, pattern_name: str, pattern_data: Dict[str, Any],
                                   training_examples: Optional[List[Dict[str, Any]]] = None) -> float:
        """
        Calcul avanc√© de la robustesse
        """
        robustness_score = 0.0

        # 1. Test de r√©sistance au bruit
        noise_resistance = self._test_noise_resistance_advanced(pattern_name, pattern_data)
        robustness_score += noise_resistance * 0.25

        # 2. Test de r√©sistance aux variations de dimensions
        dimension_resistance = self._test_dimension_resistance_advanced(pattern_name, pattern_data)
        robustness_score += dimension_resistance * 0.25

        # 3. Test de coh√©rence interne
        internal_consistency = self._test_internal_consistency_advanced(pattern_data)
        robustness_score += internal_consistency * 0.3

        # 4. Test de stabilit√© temporelle
        temporal_stability = self._test_temporal_stability(pattern_name, pattern_data)
        robustness_score += temporal_stability * 0.2

        return max(0, min(robustness_score, 1.0))

    def _test_noise_resistance_advanced(self, pattern_name: str, pattern_data: Dict[str, Any]) -> float:
        """
        Test avanc√© de r√©sistance au bruit
        """
        # Simuler diff√©rents niveaux de bruit
        noise_levels = [0.1, 0.2, 0.3]  # 10%, 20%, 30% de bruit
        resistance_scores = []

        for noise_level in noise_levels:
            # Simuler l'effet du bruit sur le score du pattern
            noise_impact = noise_level * 0.5  # Impact estim√© du bruit
            noisy_score = pattern_data.get('score', 0) * (1 - noise_impact)

            # Un pattern robuste maintient un score acceptable
            if noisy_score > 0.4:
                resistance_scores.append(1.0 - noise_level)
            elif noisy_score > 0.2:
                resistance_scores.append(0.7 - noise_level)
            else:
                resistance_scores.append(0.3 - noise_level)

        if resistance_scores:
            return statistics.mean(resistance_scores)

        return 0.7  # Score par d√©faut pour les patterns non test√©s

    def _test_dimension_resistance_advanced(self, pattern_name: str, pattern_data: Dict[str, Any]) -> float:
        """
        Test de r√©sistance aux variations de dimensions
        """
        pattern_type = pattern_name.split('.')[-1]

        # Patterns plus r√©sistants aux variations de dimensions
        dimension_resistant_patterns = [
            'color_mapping', 'repetition', 'filling', 'symmetry'
        ]

        if pattern_type in dimension_resistant_patterns:
            return 0.9
        elif pattern_type in ['rotation', 'homothety', 'reflection']:
            return 0.7
        else:
            return 0.6

    def _test_internal_consistency_advanced(self, pattern_data: Dict[str, Any]) -> float:
        """
        Test de coh√©rence interne avanc√©
        """
        details = pattern_data.get('details', {})
        consistency_score = 0.0

        if isinstance(details, dict):
            # V√©rifier la pr√©sence d'informations coh√©rentes
            if len(details) == 0:
                return 0.4  # Peu d'informations

            consistency_indicators = 0
            total_indicators = 0

            # V√©rifier les ratios num√©riques
            for key, value in details.items():
                if 'ratio' in key.lower() and isinstance(value, (int, float)):
                    if 0 < value < 10:  # Ratio raisonnable
                        consistency_indicators += 1
                    total_indicators += 1

            # V√©rifier les listes de valeurs
            for key, value in details.items():
                if isinstance(value, list) and len(value) > 1:
                    # V√©rifier si la liste est coh√©rente (pas de valeurs aberrantes)
                    if len(set(value)) <= len(value) * 0.8:  # Au moins 20% de r√©p√©titions
                        consistency_indicators += 1
                    total_indicators += 1

            if total_indicators > 0:
                consistency_score = consistency_indicators / total_indicators
            else:
                consistency_score = 0.8 if len(details) > 2 else 0.6

        return consistency_score

    def _test_temporal_stability(self, pattern_name: str, pattern_data: Dict[str, Any]) -> float:
        """
        Test de stabilit√© temporelle (coh√©rence historique)
        """
        if pattern_name not in self.evaluation_history:
            return 0.8  # Score neutre pour les nouveaux patterns

        historical_scores = self.evaluation_history[pattern_name]
        if len(historical_scores) < 2:
            return 0.8

        # Calculer la variance des scores historiques
        recent_scores = [h.get('final_score', 0) for h in historical_scores[-5:]]  # Derniers 5

        if len(recent_scores) >= 2:
            variance = statistics.variance(recent_scores)
            # Plus la variance est faible, plus le pattern est stable
            stability_score = max(0, 1.0 - variance * 2)
            return stability_score

        return 0.8

    def _compute_stability_metrics(self, pattern_name: str, pattern_data: Dict[str, Any],
                                 training_examples: Optional[List[Dict[str, Any]]] = None) -> Dict[str, float]:
        """
        Calcul des m√©triques de stabilit√©
        """
        metrics = {}

        # Variance du score sur diff√©rents exemples
        if training_examples and len(training_examples) >= 3:
            scores_on_examples = []
            for example in training_examples[:5]:  # Limiter √† 5 exemples
                # Simuler l'application du pattern
                simulated_score = pattern_data.get('score', 0) * (0.9 + 0.1 * (len(example.get('input', [])) % 3) / 2)
                scores_on_examples.append(simulated_score)

            if len(scores_on_examples) >= 2:
                metrics['score_variance'] = statistics.variance(scores_on_examples)
                metrics['score_consistency'] = 1.0 - min(metrics['score_variance'], 0.5)
            else:
                metrics['score_variance'] = 0
                metrics['score_consistency'] = 1.0
        else:
            metrics['score_variance'] = 0
            metrics['score_consistency'] = 0.8

        return metrics

    def _compute_performance_metrics(self, pattern_name: str, pattern_data: Dict[str, Any],
                                   training_examples: Optional[List[Dict[str, Any]]] = None) -> Dict[str, float]:
        """
        Calcul des m√©triques de performance
        """
        metrics = {}

        # M√©trique d'efficacit√© (score vs complexit√©)
        base_score = pattern_data.get('score', 0)
        details_count = len(pattern_data.get('details', {}))
        efficiency = base_score / max(details_count, 1)
        metrics['efficiency_score'] = min(efficiency, 1.0)

        # M√©trique de fiabilit√© (confidence vs score)
        confidence = pattern_data.get('confidence', 0)
        reliability = (base_score + confidence) / 2
        metrics['reliability_score'] = reliability

        # M√©trique de potentiel d'am√©lioration
        current_score = pattern_data.get('score', 0)
        if pattern_name in self.evaluation_history:
            historical_max = max([h.get('final_score', 0) for h in self.evaluation_history[pattern_name]])
            improvement_potential = max(0, historical_max - current_score)
        else:
            improvement_potential = 0.3  # Potentiel par d√©faut

        metrics['improvement_potential'] = improvement_potential

        return metrics

    def _analyze_overfitting_risk_advanced(self, metrics: Dict[str, float],
                                         training_examples: Optional[List[Dict[str, Any]]] = None) -> Dict[str, Any]:
        """
        Analyse avanc√©e du risque de surapprentissage
        """
        analysis = {
            'overall_risk': 0,
            'risk_factors': [],
            'confidence_intervals': {},
            'recommendations': []
        }

        risk_score = 0.0

        # Facteur 1: √âcart g√©n√©ralisation vs performance
        generalization = metrics.get('generalization_score', 0)
        base_score = metrics.get('raw_score', metrics.get('score', 0))

        if base_score > 0.8 and generalization < 0.4:
            risk_score += 0.4
            analysis['risk_factors'].append("√âcart important entre performance et g√©n√©ralisation")

        # Facteur 2: Simplicit√© trop faible
        simplicity = metrics.get('simplicity_score', 0)
        if simplicity < 0.5:
            risk_score += 0.3
            analysis['risk_factors'].append("Pattern trop complexe")

        # Facteur 3: Manque de donn√©es d'entra√Ænement
        if training_examples and len(training_examples) < 3:
            risk_score += 0.2
            analysis['risk_factors'].append("Donn√©es d'entra√Ænement insuffisantes")

        # Facteur 4: Variance des scores √©lev√©e
        score_variance = metrics.get('score_variance', 0)
        if score_variance > 0.3:
            risk_score += 0.2
            analysis['risk_factors'].append("Scores instables sur diff√©rents exemples")

        # Facteur 5: Potentiel d'am√©lioration faible
        improvement_potential = metrics.get('improvement_potential', 0)
        if improvement_potential < 0.1:
            risk_score += 0.1
            analysis['risk_factors'].append("Peu de marge d'am√©lioration")

        analysis['overall_risk'] = min(risk_score, 1.0)

        # Niveau de risque
        if analysis['overall_risk'] > 0.7:
            analysis['risk_level'] = 'CRITIQUE'
        elif analysis['overall_risk'] > 0.5:
            analysis['risk_level'] = '√âLEV√â'
        elif analysis['overall_risk'] > 0.3:
            analysis['risk_level'] = 'MOYEN'
        else:
            analysis['risk_level'] = 'FAIBLE'

        return analysis

    def _perform_cross_validation(self, pattern_name: str, pattern_data: Dict[str, Any],
                                training_examples: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        Validation crois√©e avanc√©e
        """
        if len(training_examples) < self.cross_validation_folds:
            return {'status': 'insufficient_data'}

        fold_size = len(training_examples) // self.cross_validation_folds
        cv_scores = []

        for fold in range(self.cross_validation_folds):
            # Cr√©er les sets de test et d'entra√Ænement pour ce fold
            test_start = fold * fold_size
            test_end = test_start + fold_size

            test_set = training_examples[test_start:test_end]
            train_set = training_examples[:test_start] + training_examples[test_end:]

            # √âvaluer le pattern sur le set de test
            fold_score = self._evaluate_pattern_on_test_set(
                pattern_name, pattern_data, test_set
            )
            cv_scores.append(fold_score)

        # Analyser les r√©sultats de validation crois√©e
        cv_results = {
            'status': 'completed',
            'fold_scores': cv_scores,
            'mean_score': statistics.mean(cv_scores) if cv_scores else 0,
            'std_score': statistics.stdev(cv_scores) if len(cv_scores) > 1 else 0,
            'cv_confidence': self._compute_cv_confidence(cv_scores)
        }

        return cv_results

    def _evaluate_pattern_on_test_set(self, pattern_name: str, pattern_data: Dict[str, Any],
                                    test_set: List[Dict[str, Any]]) -> float:
        """
        √âvaluer un pattern sur un set de test
        """
        if not test_set:
            return 0

        test_scores = []
        for example in test_set:
            # Simuler l'application du pattern
            simulated_confidence = pattern_data.get('confidence', 0) * 0.8  # L√©g√®re d√©gradation
            test_scores.append(simulated_confidence)

        return statistics.mean(test_scores) if test_scores else 0

    def _compute_cv_confidence(self, cv_scores: List[float]) -> float:
        """
        Calculer la confiance dans les r√©sultats de validation crois√©e
        """
        if len(cv_scores) < 2:
            return 0.5

        # Plus les scores sont coh√©rents, plus la confiance est √©lev√©e
        mean_score = statistics.mean(cv_scores)
        std_score = statistics.stdev(cv_scores)

        if std_score == 0:
            return 1.0  # Scores parfaitement coh√©rents

        # Coefficient de variation
        cv = std_score / mean_score if mean_score > 0 else 0

        # Convertir en score de confiance
        confidence = max(0, 1.0 - cv)

        return confidence

    def _test_on_validation_set(self, pattern_name: str, pattern_data: Dict[str, Any],
                              validation_examples: List[Dict[str, Any]]) -> float:
        """
        Tester le pattern sur un set de validation
        """
        if not validation_examples:
            return 0

        validation_scores = []
        for example in validation_examples:
            # Simuler l'application sur donn√©es non vues
            base_confidence = pattern_data.get('confidence', 0)
            # D√©gradation plus importante sur donn√©es non vues
            validation_confidence = base_confidence * 0.7
            validation_scores.append(validation_confidence)

        return statistics.mean(validation_scores) if validation_scores else 0

    def _compute_cross_validation_score(self, pattern_name: str, pattern_data: Dict[str, Any],
                                      training_examples: List[Dict[str, Any]]) -> float:
        """
        Calculer le score de validation crois√©e
        """
        if len(training_examples) < 2:
            return pattern_data.get('confidence', 0)

        # Simulation simple de validation crois√©e
        cv_scores = []
        for i in range(min(3, len(training_examples))):  # Maximum 3 folds
            fold_score = pattern_data.get('confidence', 0) * (0.9 - i * 0.1)
            cv_scores.append(fold_score)

        return statistics.mean(cv_scores) if cv_scores else 0

    def _compute_specificity_penalty(self, pattern_name: str, pattern_data: Dict[str, Any]) -> float:
        """
        Calculer la p√©nalit√© pour les patterns trop sp√©cifiques
        """
        penalty = 0.0

        # P√©nalit√© pour les noms trop sp√©cifiques
        specific_terms = ['specific', 'exact', 'particular', 'unique']
        for term in specific_terms:
            if term in pattern_name.lower():
                penalty += 0.2

        # P√©nalit√© pour les scores tr√®s √©lev√©s (peuvent indiquer sur-optimisation)
        if pattern_data.get('score', 0) > 0.95:
            penalty += 0.1

        return min(penalty, 0.5)

    def _compute_historical_consistency(self, pattern_name: str, current_score: float) -> float:
        """
        Calculer la coh√©rence historique
        """
        if pattern_name not in self.evaluation_history:
            return 0

        historical_scores = [h.get('final_score', 0) for h in self.evaluation_history[pattern_name][-5:]]

        if not historical_scores:
            return 0

        mean_historical = statistics.mean(historical_scores)

        # Bonus si le score actuel est coh√©rent avec l'historique
        consistency = 1.0 - abs(current_score - mean_historical)
        return max(0, consistency)

    def _get_timestamp(self) -> str:
        """Obtenir le timestamp actuel"""
        from datetime import datetime
        return datetime.now().strftime("%Y-%m-%d %H:%M:%S")

    def _update_evaluation_history(self, pattern_name: str, evaluation: Dict[str, Any]):
        """Mettre √† jour l'historique des √©valuations"""
        self.evaluation_history[pattern_name].append({
            'timestamp': evaluation['timestamp'],
            'final_score': evaluation['final_score'],
            'generalization_score': evaluation['metrics'].get('generalization_score', 0),
            'overfitting_risk': evaluation['overfitting_analysis']['overall_risk']
        })

        # Garder seulement les 10 derni√®res √©valuations
        if len(self.evaluation_history[pattern_name]) > 10:
            self.evaluation_history[pattern_name] = self.evaluation_history[pattern_name][-10:]

    def _generate_advanced_recommendations(self, evaluation: Dict[str, Any]) -> List[str]:
        """G√©n√©rer des recommandations avanc√©es"""
        recommendations = []
        metrics = evaluation['metrics']
        overfitting = evaluation['overfitting_analysis']

        # Recommandations bas√©es sur les m√©triques
        generalization = metrics.get('generalization_score', 0)
        if generalization < 0.4:
            recommendations.append("Am√©liorer la g√©n√©ralisation - collecter plus d'exemples de validation")

        simplicity = metrics.get('simplicity_score', 0)
        if simplicity < 0.5:
            recommendations.append("Simplifier le pattern - identifier les composants essentiels")

        robustness = metrics.get('robustness_score', 0)
        if robustness < 0.6:
            recommendations.append("Augmenter la robustesse - tester avec du bruit et des variations")

        # Recommandations bas√©es sur le surapprentissage
        if overfitting['overall_risk'] > 0.5:
            recommendations.append("R√©duire le risque de surapprentissage - r√©gularisation et simplification")

        # Recommandations bas√©es sur la validation crois√©e
        validation = evaluation.get('validation_results', {})
        if validation.get('status') == 'completed':
            cv_confidence = validation.get('cv_confidence', 0)
            if cv_confidence < 0.6:
                recommendations.append("R√©sultats de validation crois√©e instables - besoin de plus de donn√©es")

        # Recommandations bas√©es sur l'historique
        pattern_name = evaluation['pattern_name']
        if pattern_name in self.evaluation_history and len(self.evaluation_history[pattern_name]) > 3:
            recent_scores = [h['final_score'] for h in self.evaluation_history[pattern_name][-3:]]
            if statistics.stdev(recent_scores) > 0.2:
                recommendations.append("Scores instables historiquement - am√©liorer la stabilit√©")

        return recommendations

    def _generate_advanced_warnings(self, evaluation: Dict[str, Any]) -> List[str]:
        """G√©n√©rer des avertissements avanc√©s"""
        warnings = []
        metrics = evaluation['metrics']
        overfitting = evaluation['overfitting_analysis']

        # Avertissements critiques
        if overfitting['overall_risk'] > 0.8:
            warnings.append("ALERTE CRITIQUE: Risque de surapprentissage tr√®s √©lev√©")

        # Avertissements sur les m√©triques
        generalization = metrics.get('generalization_score', 0)
        score_consistency = metrics.get('score_consistency', 1)

        if generalization < 0.3 and score_consistency > 0.8:
            warnings.append("ATTENTION: Pattern performant mais peu g√©n√©ralisable")

        if metrics.get('improvement_potential', 0) < 0.05:
            warnings.append("NOTE: Pattern proche de son optimum - peu de marge d'am√©lioration")

        # Avertissements sur la validation
        validation = evaluation.get('validation_results', {})
        if validation.get('status') == 'insufficient_data':
            warnings.append("ATTENTION: Donn√©es insuffisantes pour validation fiable")

        return warnings

    def _generate_improvement_suggestions(self, evaluation: Dict[str, Any]) -> List[str]:
        """G√©n√©rer des suggestions d'am√©lioration"""
        suggestions = []
        metrics = evaluation['metrics']

        # Suggestions bas√©es sur les m√©triques faibles
        if metrics.get('generalization_score', 0) < 0.5:
            suggestions.append("Exp√©rimenter avec des transformations plus g√©n√©riques")

        if metrics.get('simplicity_score', 0) < 0.6:
            suggestions.append("D√©composer le pattern en composants plus simples")

        if metrics.get('robustness_score', 0) < 0.7:
            suggestions.append("Ajouter des tests de r√©sistance aux variations")

        # Suggestions bas√©es sur l'historique
        pattern_name = evaluation['pattern_name']
        if pattern_name in self.evaluation_history:
            historical_avg = statistics.mean([h['final_score'] for h in self.evaluation_history[pattern_name]])
            current_score = evaluation['final_score']

            if current_score < historical_avg:
                suggestions.append("Score en baisse par rapport √† l'historique - analyser les causes")

        return suggestions

    def _compute_final_score_with_trends(self, evaluation: Dict[str, Any]) -> float:
        """Calculer le score final avec analyse des tendances"""
        metrics = evaluation['metrics']
        overfitting_risk = evaluation['overfitting_analysis']['overall_risk']

        # Score de base pond√©r√©
        base_score = (
            metrics.get('generalization_score', 0) * self.generalization_weight +
            metrics.get('simplicity_score', 0) * self.simplicity_weight +
            metrics.get('robustness_score', 0) * self.robustness_weight
        )

        # P√©nalit√© pour le surapprentissage
        penalty = overfitting_risk * 0.4

        # Ajustement bas√© sur les m√©triques de performance
        performance_bonus = metrics.get('reliability_score', 0) * 0.1

        final_score = base_score - penalty + performance_bonus

        return max(0, min(final_score, 1.0))

    def evaluate_pattern_suite_advanced(self, patterns_analysis: Dict[str, Any],
                                      training_examples: Optional[List[Dict[str, Any]]] = None,
                                      validation_examples: Optional[List[Dict[str, Any]]] = None) -> Dict[str, Any]:
        """
        √âvaluation avanc√©e d'une suite compl√®te de patterns
        """
        evaluation = {
            'overall_quality': 0,
            'pattern_diversity': 0,
            'overfitting_risk': 0,
            'recommended_patterns': [],
            'pattern_conflicts': [],
            'quality_report': [],
            'system_recommendations': [],
            'improvement_plan': []
        }

        if not patterns_analysis:
            return evaluation

        # √âvaluer chaque pattern individuellement
        scored_patterns = []
        pattern_categories = ['spatial', 'color', 'structural', 'mathematical']

        for category in pattern_categories:
            if category in patterns_analysis:
                for pattern_name, pattern_data in patterns_analysis[category].items():
                    full_pattern_name = f"{category}.{pattern_name}"
                    pattern_evaluation = self.evaluate_pattern_comprehensive_advanced(
                        full_pattern_name, pattern_data, None, None,
                        training_examples, validation_examples
                    )
                    scored_patterns.append(pattern_evaluation)

        if scored_patterns:
            # Calcul de la qualit√© globale
            avg_quality = statistics.mean([p['final_score'] for p in scored_patterns])
            evaluation['overall_quality'] = avg_quality

            # Diversit√© des patterns
            categories_used = set(p['pattern_name'].split('.')[0] for p in scored_patterns)
            evaluation['pattern_diversity'] = len(categories_used) / len(pattern_categories)

            # Risque global de surapprentissage
            avg_overfitting_risk = statistics.mean([p['overfitting_analysis']['overall_risk'] for p in scored_patterns])
            evaluation['overfitting_risk'] = avg_overfitting_risk

            # Patterns recommand√©s (score final > 0.6)
            recommended = [p['pattern_name'] for p in scored_patterns if p['final_score'] > 0.6]
            evaluation['recommended_patterns'] = recommended

            # D√©tection de conflits
            evaluation['pattern_conflicts'] = self._detect_pattern_conflicts(scored_patterns)

            # Rapport de qualit√© d√©taill√©
            evaluation['quality_report'] = [
                ".2f",
                ".2f",
                ".2f",
                f"Patterns analys√©s: {len(scored_patterns)}",
                f"Patterns recommand√©s: {len(recommended)}",
                f"Conflits d√©tect√©s: {len(evaluation['pattern_conflicts'])}"
            ]

            # Recommandations syst√®me
            evaluation['system_recommendations'] = self._generate_system_recommendations(evaluation)

            # Plan d'am√©lioration
            evaluation['improvement_plan'] = self._generate_improvement_plan(scored_patterns)

        return evaluation

    def _detect_pattern_conflicts(self, scored_patterns: List[Dict[str, Any]]) -> List[str]:
        """D√©tecter les conflits entre patterns"""
        conflicts = []

        # Analyser les risques de surapprentissage √©lev√©s
        high_risk_patterns = [p for p in scored_patterns if p['overfitting_analysis']['overall_risk'] > 0.6]

        if len(high_risk_patterns) > 2:
            conflicts.append(f"Multiples patterns √† risque √©lev√©: {len(high_risk_patterns)}")

        # Analyser les scores tr√®s variables
        if scored_patterns:
            scores = [p['final_score'] for p in scored_patterns]
            if len(scores) > 1 and statistics.stdev(scores) > 0.3:
                conflicts.append("Scores tr√®s variables entre patterns")

        return conflicts

    def _generate_system_recommendations(self, evaluation: Dict[str, Any]) -> List[str]:
        """G√©n√©rer des recommandations pour le syst√®me complet"""
        recommendations = []

        if evaluation['overall_quality'] < 0.5:
            recommendations.append("Qualit√© globale faible - focus sur l'am√©lioration des patterns de base")

        if evaluation['pattern_diversity'] < 0.5:
            recommendations.append("Diversit√© des patterns insuffisante - d√©velopper d'autres cat√©gories")

        if evaluation['overfitting_risk'] > 0.4:
            recommendations.append("Risque de surapprentissage syst√®me √©lev√© - impl√©menter r√©gularisation globale")

        if len(evaluation['recommended_patterns']) < 3:
            recommendations.append("Peu de patterns recommand√©s - d√©velopper des patterns plus robustes")

        return recommendations

    def _generate_improvement_plan(self, scored_patterns: List[Dict[str, Any]]) -> List[str]:
        """G√©n√©rer un plan d'am√©lioration"""
        plan = []

        # Prioriser les am√©liorations
        low_quality_patterns = [p for p in scored_patterns if p['final_score'] < 0.4]
        if low_quality_patterns:
            plan.append(f"Am√©liorer {len(low_quality_patterns)} patterns de basse qualit√©")

        # Identifier les patterns √† risque
        high_risk_patterns = [p for p in scored_patterns if p['overfitting_analysis']['overall_risk'] > 0.5]
        if high_risk_patterns:
            plan.append(f"Traiter {len(high_risk_patterns)} patterns √† risque de surapprentissage")

        # Recommandations g√©n√©rales
        if len(scored_patterns) < 5:
            plan.append("D√©velopper plus de patterns pour am√©liorer la couverture")

        plan.append("Impl√©menter m√©triques de monitoring continu")
        plan.append("Ajouter tests de validation crois√©e automatiques")

        return plan

    def generate_comprehensive_report(self, evaluation: Dict[str, Any]) -> str:
        """G√©n√©rer un rapport complet de l'√©valuation"""
        report = ".2f"
        report += "\n" + "=" * 80

        # M√©triques principales
        metrics = evaluation['metrics']
        report += ".2f"
        report += ".2f"
        report += ".2f"
        report += ".2f"
        report += ".2f"
        report += ".2f"
        report += ".2f"

        # Analyse du surapprentissage
        overfitting = evaluation['overfitting_analysis']
        report += ".2f"
        report += f"  Niveau de risque: {overfitting['risk_level']}\n"
        if overfitting['risk_factors']:
            report += "  Facteurs de risque:\n"
            for factor in overfitting['risk_factors']:
                report += f"    ‚Ä¢ {factor}\n"

        # R√©sultats de validation
        validation = evaluation.get('validation_results', {})
        if validation.get('status') == 'completed':
            report += ".2f"
            report += ".2f"
            report += ".2f"
        else:
            report += f"  Statut: {validation.get('status', 'unknown')}\n"

        # Recommandations
        if evaluation['recommendations']:
            report += "\nüìã RECOMMANDATIONS:\n"
            for i, rec in enumerate(evaluation['recommendations'], 1):
                report += f"  {i}. {rec}\n"

        # Avertissements
        if evaluation['warnings']:
            report += "\n‚ö†Ô∏è AVERTISSEMENTS:\n"
            for warning in evaluation['warnings']:
                report += f"  ‚Ä¢ {warning}\n"

        # Suggestions d'am√©lioration
        if evaluation['improvement_suggestions']:
            report += "\nüí° SUGGESTIONS D'AM√âLIORATION:\n"
            for suggestion in evaluation['improvement_suggestions']:
                report += f"  ‚Ä¢ {suggestion}\n"

        return report
