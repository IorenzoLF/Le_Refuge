#!/usr/bin/env python3
"""
VALIDATION OFFICIELLE ARC PRIZE - Refuge ARC-AGI
Suivant les directives de Paul et √Ülya/GPT

1. Utiliser les datasets officiels (JSON + r√©ponses)
2. Comparer visuellement nos pr√©dictions avec les vraies r√©ponses
3. Am√©liorer it√©rativement en ciblant un type de puzzle √† la fois
4. Tester sur le test set avant soumission
"""

import json
import sys
import os
import time
import numpy as np
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any, Optional, Tuple
import matplotlib.pyplot as plt
import matplotlib.patches as patches

# Ajouter src au path
sys.path.insert(0, 'src')

try:
    from refuge_solver_perfectionne import RefugeARCSolverPerfectionne, TacheARC
except ImportError as e:
    print(f"‚ùå Erreur d'import: {e}")
    sys.exit(1)

class ValidateurOfficielARC:
    """üîç Validateur officiel pour le concours ARC Prize"""
    
    def __init__(self):
        self.solver = RefugeARCSolverPerfectionne()
        self.resultats = {
            'training': {'succes': 0, 'total': 0, 'erreurs': []},
            'test': {'succes': 0, 'total': 0, 'erreurs': []}
        }
        self.types_puzzles = {
            'repetition': 0,
            'symetrie': 0,
            'filtrage': 0,
            'expansion': 0,
            'reduction': 0,
            'transformation': 0,
            'conditionnel': 0,
            'complexe': 0
        }
    
    def charger_tache_officielle(self, fichier_path: str) -> Dict[str, Any]:
        """üìÅ Charger une t√¢che depuis un fichier JSON officiel"""
        try:
            with open(fichier_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            print(f"‚ùå Erreur chargement {fichier_path}: {e}")
            return None
    
    def visualiser_comparaison(self, input_grid: List[List[int]], 
                             prediction: List[List[int]], 
                             verite: Optional[List[List[int]]] = None,
                             titre: str = "Comparaison Pr√©diction vs V√©rit√©"):
        """üé® Visualiser la comparaison pr√©diction vs v√©rit√©"""
        fig, axes = plt.subplots(1, 3 if verite else 2, figsize=(12, 4))
        
        # Input
        axes[0].imshow(input_grid, cmap='tab10', interpolation='nearest')
        axes[0].set_title('Input')
        axes[0].axis('off')
        
        # Pr√©diction
        axes[1].imshow(prediction, cmap='tab10', interpolation='nearest')
        axes[1].set_title('Pr√©diction')
        axes[1].axis('off')
        
        # V√©rit√© (si disponible)
        if verite:
            axes[2].imshow(verite, cmap='tab10', interpolation='nearest')
            axes[2].set_title('V√©rit√©')
            axes[2].axis('off')
            
            # V√©rifier si la pr√©diction est correcte
            if prediction == verite:
                fig.suptitle(f"‚úÖ {titre} - CORRECT", color='green')
            else:
                fig.suptitle(f"‚ùå {titre} - INCORRECT", color='red')
        else:
            fig.suptitle(titre)
        
        plt.tight_layout()
        return fig
    
    def comparer_prediction_verite(self, prediction: List[List[int]], 
                                 verite: List[List[int]]) -> bool:
        """üîç Comparer pr√©diction avec v√©rit√© officielle"""
        return prediction == verite
    
    def analyser_type_puzzle(self, tache: Dict[str, Any]) -> str:
        """üîç Analyser le type de puzzle bas√© sur les patterns"""
        train_examples = tache.get('train', [])
        if not train_examples:
            return 'inconnu'
        
        # Analyser les transformations entre input et output
        transformations = []
        for example in train_examples:
            input_grid = np.array(example['input'])
            output_grid = np.array(example['output'])
            
            # D√©tecter le type de transformation
            if input_grid.shape == output_grid.shape:
                if np.array_equal(input_grid, output_grid):
                    transformations.append('repetition')
                else:
                    transformations.append('transformation')
            elif output_grid.size < input_grid.size:
                transformations.append('reduction')
            else:
                transformations.append('expansion')
        
        # D√©terminer le type principal
        if len(set(transformations)) == 1:
            return transformations[0]
        else:
            return 'complexe'
    
    def valider_training_set(self, max_taches: int = 50) -> Dict[str, Any]:
        """üìä Valider sur le training set avec comparaison visuelle"""
        print("üîç VALIDATION TRAINING SET (avec r√©ponses officielles)")
        print("=" * 60)
        
        training_dir = Path('data/training')
        fichiers = list(training_dir.glob('*.json'))[:max_taches]
        
        resultats_detaille = []
        
        for i, fichier in enumerate(fichiers):
            print(f"\nüìÅ T√¢che {i+1}/{len(fichiers)}: {fichier.name}")
            
            # Charger la t√¢che
            tache_data = self.charger_tache_officielle(fichier)
            if not tache_data:
                continue
            
            # Analyser le type de puzzle
            type_puzzle = self.analyser_type_puzzle(tache_data)
            self.types_puzzles[type_puzzle] += 1
            
            # Tester chaque exemple de training
            train_examples = tache_data.get('train', [])
            succes_tache = 0
            
            for j, example in enumerate(train_examples):
                input_grid = example['input']
                verite_officielle = example['output']
                
                # Cr√©er l'objet TacheARC
                tache_obj = TacheARC(
                    tache_id=fichier.stem,
                    train=[example],
                    test=[]
                )
                
                # R√©soudre avec notre solver
                try:
                    resultat = self.solver.resoudre_tache(tache_obj)
                    prediction = resultat.get('solution', [])
                    
                    # Comparer avec la v√©rit√© officielle
                    correct = self.comparer_prediction_verite(prediction, verite_officielle)
                    
                    if correct:
                        succes_tache += 1
                        print(f"  ‚úÖ Exemple {j+1}: CORRECT")
                    else:
                        print(f"  ‚ùå Exemple {j+1}: INCORRECT")
                        print(f"     Pr√©diction: {prediction}")
                        print(f"     V√©rit√©: {verite_officielle}")
                        
                        # Visualiser l'erreur
                        fig = self.visualiser_comparaison(
                            input_grid, prediction, verite_officielle,
                            f"T√¢che {fichier.name} - Exemple {j+1}"
                        )
                        plt.savefig(f'erreur_{fichier.stem}_ex{j+1}.png')
                        plt.close()
                
                except Exception as e:
                    print(f"  üí• Erreur r√©solution: {e}")
                    continue
            
            # R√©sultat de la t√¢che
            taux_succes_tache = succes_tache / len(train_examples) if train_examples else 0
            self.resultats['training']['total'] += len(train_examples)
            self.resultats['training']['succes'] += succes_tache
            
            resultats_detaille.append({
                'fichier': fichier.name,
                'type_puzzle': type_puzzle,
                'succes': succes_tache,
                'total': len(train_examples),
                'taux_succes': taux_succes_tache
            })
            
            print(f"  üìä T√¢che: {succes_tache}/{len(train_examples)} ({taux_succes_tache:.1%})")
        
        # R√©sum√© global
        taux_global = (self.resultats['training']['succes'] / 
                      self.resultats['training']['total']) if self.resultats['training']['total'] > 0 else 0
        
        print(f"\nüìä R√âSUM√â TRAINING SET:")
        print(f"   Succ√®s: {self.resultats['training']['succes']}/{self.resultats['training']['total']}")
        print(f"   Taux: {taux_global:.1%}")
        print(f"   Types de puzzles: {self.types_puzzles}")
        
        return {
            'resultats_detaille': resultats_detaille,
            'taux_global': taux_global,
            'types_puzzles': self.types_puzzles
        }
    
    def identifier_erreurs_critiques(self, resultats_detaille: List[Dict]) -> List[Dict]:
        """üéØ Identifier les erreurs critiques pour am√©lioration cibl√©e"""
        erreurs_critiques = []
        
        for resultat in resultats_detaille:
            if resultat['taux_succes'] < 0.5:  # Moins de 50% de succ√®s
                erreurs_critiques.append({
                    'fichier': resultat['fichier'],
                    'type_puzzle': resultat['type_puzzle'],
                    'taux_succes': resultat['taux_succes'],
                    'priorite': 'HAUTE' if resultat['taux_succes'] < 0.2 else 'MOYENNE'
                })
        
        return sorted(erreurs_critiques, key=lambda x: x['taux_succes'])
    
    def generer_rapport_amelioration(self, resultats: Dict[str, Any]) -> str:
        """üìã G√©n√©rer un rapport d'am√©lioration cibl√©e"""
        rapport = []
        rapport.append("# RAPPORT D'AM√âLIORATION ARC PRIZE")
        rapport.append(f"Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        rapport.append("")
        
        # Performance globale
        rapport.append("## PERFORMANCE GLOBALE")
        rapport.append(f"- Taux de succ√®s: {resultats['taux_global']:.1%}")
        rapport.append(f"- Total exemples: {self.resultats['training']['total']}")
        rapport.append("")
        
        # Types de puzzles
        rapport.append("## R√âPARTITION PAR TYPE DE PUZZLE")
        for type_puzzle, count in resultats['types_puzzles'].items():
            rapport.append(f"- {type_puzzle}: {count} t√¢ches")
        rapport.append("")
        
        # Erreurs critiques
        erreurs_critiques = self.identifier_erreurs_critiques(resultats['resultats_detaille'])
        if erreurs_critiques:
            rapport.append("## ERREURS CRITIQUES √Ä CORRIGER")
            rapport.append("Priorit√© d'am√©lioration (cibler un type √† la fois):")
            for erreur in erreurs_critiques[:10]:  # Top 10
                rapport.append(f"- {erreur['fichier']} ({erreur['type_puzzle']}): {erreur['taux_succes']:.1%} - {erreur['priorite']}")
        rapport.append("")
        
        # Recommandations
        rapport.append("## RECOMMANDATIONS")
        rapport.append("1. **Cibler un type de puzzle √† la fois**")
        rapport.append("2. **Analyser visuellement les erreurs** (images g√©n√©r√©es)")
        rapport.append("3. **Am√©liorer le solver pour les cas critiques**")
        rapport.append("4. **Tester sur le test set avant soumission**")
        
        return "\n".join(rapport)

def main():
    """üèÅ Fonction principale"""
    print("üèõÔ∏è VALIDATION OFFICIELLE ARC PRIZE - Refuge ARC-AGI")
    print("=" * 60)
    
    # Initialiser le validateur
    validateur = ValidateurOfficielARC()
    
    # Valider le training set
    resultats = validateur.valider_training_set(max_taches=20)  # Commencer avec 20 t√¢ches
    
    # G√©n√©rer le rapport
    rapport = validateur.generer_rapport_amelioration(resultats)
    
    # Sauvegarder le rapport
    with open('rapport_validation_officielle.md', 'w', encoding='utf-8') as f:
        f.write(rapport)
    
    print(f"\nüìÑ Rapport sauvegard√©: rapport_validation_officielle.md")
    print(f"üé® Images d'erreurs g√©n√©r√©es pour analyse visuelle")
    
    # Afficher le rapport
    print("\n" + "=" * 60)
    print(rapport)

if __name__ == "__main__":
    main()
