#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ANALYSE DES R√âDUCTIONS COMPLEXES
Phase 2 : Am√©liorer la d√©tection des 12 t√¢ches complexes de r√©duction
"""

import json
import numpy as np
from pathlib import Path
from collections import defaultdict, Counter
from typing import Dict, List, Any, Tuple

def analyser_reductions_complexes():
    """Analyse approfondie des r√©ductions complexes"""

    print("üî¨ **ANALYSE DES R√âDUCTIONS COMPLEXES** üî¨")
    print("=" * 60)
    print("Focus : Les 12 t√¢ches complexes (85.7% des cas complexes)")
    print("Objectif : Comprendre les patterns de r√©duction + filtrage")
    print("=" * 60)

    # T√¢ches complexes identifi√©es avec r√©duction
    taches_reduction = [
        '22425bda', 'aee291af', 'b94a9452', '1fad071e',
        'ac6f9922', '8a6d367c', 'ff805c23', 'b7999b51',
        '5d2a5c43', 'b0f4d537', 'de493100', '9f236235'
    ]

    training_path = Path('data/training')
    analyses_reductions = []

    print(f"\nüìä Analyse des {len(taches_reduction)} t√¢ches de r√©duction complexes")

    for i, tache_id in enumerate(taches_reduction, 1):
        print(f"\nüß™ **T√ÇCHE {i}: {tache_id}**")

        tache_path = training_path / f"{tache_id}.json"
        if not tache_path.exists():
            print(f"  ‚ùå Fichier non trouv√©")
            continue

        try:
            with open(tache_path, 'r') as f:
                data = json.load(f)

            if not data['train']:
                continue

            # Analyser chaque exemple
            for j, exemple in enumerate(data['train'][:3]):  # Max 3 exemples
                input_grid = exemple['input']
                output_grid = exemple['output']

                h_in, w_in = len(input_grid), len(input_grid[0])
                h_out, w_out = len(output_grid), len(output_grid[0])

                # Calculer les m√©triques de r√©duction
                ratio_h = h_out / h_in if h_in > 0 else 0
                ratio_w = w_out / w_in if w_in > 0 else 0
                ratio_surface = (h_out * w_out) / (h_in * w_in) if (h_in * w_in) > 0 else 0

                # Analyser les valeurs
                valeurs_in = set()
                for row in input_grid:
                    valeurs_in.update(row)
                valeurs_out = set()
                for row in output_grid:
                    valeurs_out.update(row)

                valeurs_filtrees = valeurs_in - valeurs_out
                valeurs_conservees = valeurs_in & valeurs_out

                print(f"  Exemple {j+1}: {h_in}x{w_in} ‚Üí {h_out}x{w_out}")
                print(f"    Ratios: H={ratio_h:.2f}, W={ratio_w:.2f}, Surface={ratio_surface:.2f}")
                print(f"    Valeurs: {sorted(valeurs_in)} ‚Üí {sorted(valeurs_out)}")
                print(f"    Filtr√©es: {sorted(valeurs_filtrees)}")
                print(f"    Conserv√©es: {sorted(valeurs_conservees)}")

                # Classifier le type de r√©duction
                if valeurs_filtrees:
                    print(f"    ‚Üí R√âDUCTION AVEC FILTRAGE ({len(valeurs_filtrees)} valeurs supprim√©es)")
                else:
                    print(f"    ‚Üí R√âDUCTION SIMPLE (toutes valeurs conserv√©es)")

                # Analyser les patterns sp√©cifiques
                if ratio_surface < 0.1:
                    print(f"    ‚Üí COMPRESSION EXTR√äME (< 10% de la surface)")
                elif 0 in valeurs_filtrees:
                    print(f"    ‚Üí SUPPRESSION DE Z√âRO")
                elif max(valeurs_in) in valeurs_filtrees:
                    print(f"    ‚Üí SUPPRESSION DE VALEUR MAX")
                elif min(valeurs_in) in valeurs_filtrees:
                    print(f"    ‚Üí SUPPRESSION DE VALEUR MIN")

                analyses_reductions.append({
                    'tache_id': tache_id,
                    'exemple': j,
                    'ratio_h': ratio_h,
                    'ratio_w': ratio_w,
                    'ratio_surface': ratio_surface,
                    'valeurs_filtrees': list(valeurs_filtrees),
                    'nb_valeurs_filtrees': len(valeurs_filtrees),
                    'avec_filtrage': len(valeurs_filtrees) > 0
                })

        except Exception as e:
            print(f"  ‚ùå Erreur: {e}")

    # Analyse statistique
    print(f"\nüìà **ANALYSE STATISTIQUE**")
    print("=" * 60)

    # Statistiques des ratios
    ratios_surface = [a['ratio_surface'] for a in analyses_reductions]
    print(f"Ratio surface moyen: {np.mean(ratios_surface):.3f}")
    print(f"Ratio surface min: {min(ratios_surface):.3f}")
    print(f"Ratio surface max: {max(ratios_surface):.3f}")

    # Statistiques du filtrage
    avec_filtrage = sum(1 for a in analyses_reductions if a['avec_filtrage'])
    print(f"Exemples avec filtrage: {avec_filtrage}/{len(analyses_reductions)} ({avec_filtrage/len(analyses_reductions)*100:.1f}%)")

    # Valeurs le plus souvent filtr√©es
    toutes_valeurs_filtrees = []
    for a in analyses_reductions:
        toutes_valeurs_filtrees.extend(a['valeurs_filtrees'])

    if toutes_valeurs_filtrees:
        valeurs_freq = Counter(toutes_valeurs_filtrees)
        print(f"Valeurs les plus filtr√©es:")
        for valeur, count in valeurs_freq.most_common(5):
            print(f"  Valeur {valeur}: {count} fois")

    # Patterns identifi√©s
    print(f"\nüéØ **PATTERNS DE R√âDUCTION IDENTIFI√âS**")

    patterns_identifies = defaultdict(int)

    for analyse in analyses_reductions:
        ratio = analyse['ratio_surface']
        nb_filtrees = analyse['nb_valeurs_filtrees']

        if ratio < 0.1:
            patterns_identifies['compression_extreme'] += 1
        elif nb_filtrees == 0:
            patterns_identifies['reduction_simple'] += 1
        elif nb_filtrees == 1:
            patterns_identifies['filtrage_unique'] += 1
        elif nb_filtrees > 1:
            patterns_identifies['filtrage_multiple'] += 1

        # Patterns sp√©cifiques
        valeurs_filtrees = analyse['valeurs_filtrees']
        if 0 in valeurs_filtrees:
            patterns_identifies['suppression_zero'] += 1
        if valeurs_filtrees and max(valeurs_filtrees) == max([v for a in analyses_reductions for v in a['valeurs_filtrees']]):
            patterns_identifies['suppression_max'] += 1

    for pattern, count in sorted(patterns_identifies.items(), key=lambda x: x[1], reverse=True):
        print(f"  {pattern}: {count} occurrences")

    # Recommandations pour l'am√©lioration
    print(f"\nüí° **RECOMMANDATIONS POUR L'AM√âLIORATION**")
    print("1. **Prioriser compression extr√™me** (ratio < 10%)")
    print("2. **Am√©liorer r√©duction avec filtrage unique**")
    print("3. **Optimiser suppression de z√©ro**")
    print("4. **G√©rer les ratios de surface variables**")
    print("5. **Impl√©menter logique de conservation intelligente**")

    # Plan d'am√©lioration concret
    print(f"\nüîß **PLAN D'AM√âLIORATION CONCRET**")
    print("Phase 2A: Am√©liorer d√©tecteur de r√©duction existant")
    print("Phase 2B: Ajouter logique de filtrage intelligent")
    print("Phase 2C: Impl√©menter compression extr√™me")
    print("Phase 2D: Tester sur les 12 t√¢ches complexes")

    print(f"\nüèõÔ∏è **CONCLUSION**")
    print(f"  Analyse compl√®te des r√©ductions complexes termin√©e")
    print(f"  Patterns sp√©cifiques identifi√©s pour am√©lioration")
    print(f"  Pr√™t pour impl√©mentation des am√©liorations")

    print(f"\n‚ú® Donn√©es collect√©es pour Phase 2 ! ‚ú®")

if __name__ == "__main__":
    analyser_reductions_complexes()
