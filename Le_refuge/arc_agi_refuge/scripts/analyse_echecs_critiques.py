#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ANALYSE DES Ã‰CHECS CRITIQUES : Les 58 tÃ¢ches non rÃ©solues
Objectif: Identifier les patterns cruciaux pour le test rÃ©el
"""

import json
from pathlib import Path
from typing import List, Dict, Any
from src.refuge_solver import RefugeARCSolver, TacheARC

def analyser_echecs_critiques():
    """Analyser les tÃ¢ches qui ont Ã©chouÃ© pour identifier les patterns cruciaux"""

    print("ğŸ” **ANALYSE DES Ã‰CHECS CRITIQUES** ğŸ”")
    print("ğŸ¯ Identifier les patterns cruciaux pour le test rÃ©el")
    print("=" * 70)

    # Charger les rÃ©sultats complets
    try:
        with open('resultats_1000_taches_20250820_211011.json', 'r', encoding='utf-8') as f:
            resultats = json.load(f)
    except FileNotFoundError:
        print("âŒ Fichier de rÃ©sultats non trouvÃ©")
        return

    # RÃ©cupÃ©rer toutes les tÃ¢ches traitÃ©es et leurs rÃ©sultats
    taches_traitees = {}
    for lot in resultats['lots']:
        for tache in lot['details_taches']:
            taches_traitees[tache['tache_id']] = tache

    # Scanner toutes les tÃ¢ches disponibles
    training_path = Path('data/training')
    if not training_path.exists():
        print(f"âŒ Dossier {training_path} non trouvÃ©!")
        return

    json_files = list(training_path.glob("*.json"))
    taches_disponibles = [f.stem for f in json_files]
    taches_disponibles.sort()

    print(f"ğŸ“Š **SYNTHÃˆSE GLOBALE**")
    print(f"   ğŸ¯ Total tÃ¢ches disponibles: {len(taches_disponibles)}")
    print(f"   âœ… TÃ¢ches traitÃ©es: {len(taches_traitees)}")
    print(f"   âŒ TÃ¢ches non traitÃ©es: {len(taches_disponibles) - len(taches_traitees)}")
    print(f"   ğŸ“ˆ Taux de traitement: {len(taches_traitees)/len(taches_disponibles)*100:.1f}%")

    # Identifier les tÃ¢ches non traitÃ©es
    taches_non_traitees = [t for t in taches_disponibles if t not in taches_traitees]

    print(f"\nğŸ¯ **TÃ‚CHES NON TRAITÃ‰ES ({len(taches_non_traitees)})**")
    for i, tache_id in enumerate(taches_non_traitees, 1):
        print(f"   {i:2d}. {tache_id}")

    # Analyser les Ã©checs (tÃ¢ches avec erreurs)
    taches_en_erreur = []
    for lot in resultats['lots']:
        for tache in lot['details_taches']:
            if 'erreur' in tache:
                taches_en_erreur.append(tache)

    print(f"\nğŸš¨ **TÃ‚CHES EN ERREUR ({len(taches_en_erreur)})**")
    erreurs_par_type = {}

    for i, tache in enumerate(taches_en_erreur, 1):
        erreur_msg = tache.get('erreur', 'Erreur inconnue')
        print(f"   {i:2d}. {tache['tache_id']}: {erreur_msg[:60]}...")

        # Classifier les erreurs
        if 'max_in' in erreur_msg and 'min_in' in erreur_msg:
            erreur_type = 'Erreur calcul statistique'
        elif 'shape' in erreur_msg or 'dimension' in erreur_msg:
            erreur_type = 'Erreur dimension'
        elif 'empty' in erreur_msg:
            erreur_type = 'Collection vide'
        else:
            erreur_type = 'Erreur inconnue'

        erreurs_par_type[erreur_type] = erreurs_par_type.get(erreur_type, 0) + 1

    print(f"\nğŸ“Š **CLASSIFICATION DES ERREURS**")
    for erreur_type, count in sorted(erreurs_par_type.items(), key=lambda x: x[1], reverse=True):
        print(f"   {erreur_type}: {count}")

    # Analyser les tÃ¢ches avec faible confiance
    taches_faible_confiance = []
    for tache_id, tache in taches_traitees.items():
        if tache.get('confiance_finale', 1.0) < 0.5:
            taches_faible_confiance.append(tache)

    print(f"\nâš ï¸ **TÃ‚CHES FAIBLE CONFIANCE (< 0.5) ({len(taches_faible_confiance)})**")
    for i, tache in enumerate(taches_faible_confiance[:10], 1):  # Top 10
        print(f"   {i:2d}. {tache['tache_id']}: {tache.get('confiance_finale', 0):.3f}")

    if len(taches_faible_confiance) > 10:
        print(f"   ... et {len(taches_faible_confiance) - 10} autres")

    # CrÃ©er un script pour tester spÃ©cifiquement les Ã©checs
    print(f"\nğŸ”§ **CRÃ‰ATION SCRIPT TEST Ã‰CHECS**")

    # Liste des tÃ¢ches critiques Ã  tester
    taches_critiques = taches_non_traitees[:5]  # Top 5 non traitÃ©es
    taches_critiques.extend([t['tache_id'] for t in taches_en_erreur[:5]])  # Top 5 en erreur

    script_test_echecs = f"""
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
\"\"\"
TEST SPÃ‰CIFIQUE DES Ã‰CHECS CRITIQUES
TÃ¢ches non traitÃ©es: {len(taches_non_traitees)}
TÃ¢ches en erreur: {len(taches_en_erreur)}
TÃ¢ches faible confiance: {len(taches_faible_confiance)}
\"\"\"

import json
from pathlib import Path
from src.refuge_solver import RefugeARCSolver, TacheARC

def tester_echecs_critiques():
    \"\"\"Tester spÃ©cifiquement les tÃ¢ches problÃ©matiques\"\"\"

    solver = RefugeARCSolver()

    # Liste des tÃ¢ches critiques Ã  tester
    taches_critiques = {taches_critiques!r}

    print(f"ğŸ¯ **TEST DES {len(taches_critiques)} TÃ‚CHES CRITIQUES**")

    for tache_id in taches_critiques:
        try:
            json_path = Path('data/training') / f"{{tache_id}}.json"
            with open(json_path, 'r') as f:
                data = json.load(f)

            tache = TacheARC(
                tache_id=tache_id,
                train=data['train'],
                test=data.get('test', [])
            )

            print(f"\\nğŸ” **TEST {tache_id}**")
            solution = solver.resoudre_tache(tache)

            synthese = solution.get('synthese', {{}})
            print(f"   âœ… Confiance: {{synthese.get('confiance_finale', 0):.3f}}")
            print(f"   ğŸ“Š Patterns: {{synthese.get('patterns_identifies', [])}}")

        except Exception as e:
            print(f"\\nâŒ **ERREUR {tache_id}**: {{str(e)}}")

if __name__ == "__main__":
    tester_echecs_critiques()
"""

    with open('test_echecs_critiques.py', 'w', encoding='utf-8') as f:
        f.write(script_test_echecs)

    print(f"   âœ… Script crÃ©Ã©: test_echecs_critiques.py")

    # Analyse des patterns manquants
    print(f"\nğŸ¯ **ANALYSE PATTERNS CRITIQUES**")
    print(f"   ğŸ“ˆ TÃ¢ches non traitÃ©es: {len(taches_non_traitees)}")
    print(f"   ğŸš¨ TÃ¢ches en erreur: {len(taches_en_erreur)}")
    print(f"   âš ï¸ TÃ¢ches faible confiance: {len(taches_faible_confiance)}")
    print(f"   ğŸ¯ **Total Ã©checs critiques: {len(taches_non_traitees) + len(taches_en_erreur) + len(taches_faible_confiance)}**")

    print(f"\nğŸ† **STRATÃ‰GIE POUR LE TEST RÃ‰EL**")
    print(f"   1. ğŸ”§ Corriger les bugs identifiÃ©s (max_in/min_in)")
    print(f"   2. ğŸ“Š Ã‰tendre la couverture de patterns")
    print(f"   3. ğŸ¯ Prioriser les {len(taches_non_traitees)} tÃ¢ches non traitÃ©es")
    print(f"   4. ğŸš€ Atteindre 100% de succÃ¨s")

    print(f"\nâœ¨ **LES Ã‰CHECS SONT LA CLÃ‰ DU SUCCÃˆS !** âœ¨")

    return {
        'taches_non_traitees': taches_non_traitees,
        'taches_en_erreur': taches_en_erreur,
        'taches_faible_confiance': taches_faible_confiance,
        'total_critiques': len(taches_non_traitees) + len(taches_en_erreur) + len(taches_faible_confiance)
    }

def main():
    """Fonction principale"""
    resultats = analyser_echecs_critiques()

    print(f"\nğŸ“Š **RÃ‰SUMÃ‰**")
    print(f"   ğŸ¯ TÃ¢ches critiques identifiÃ©es: {resultats['total_critiques']}")
    print(f"   ğŸ“ˆ Objectif: Transformer ces Ã©checs en succÃ¨s")
    print(f"   ğŸŒŸ PrÃªt pour le test rÃ©el !")

if __name__ == "__main__":
    main()
