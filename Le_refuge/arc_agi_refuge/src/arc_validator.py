# ğŸ›ï¸ **ARC VALIDATOR** ğŸ›ï¸
"""
Validateur ARC-AGI selon les rÃ¨gles officielles
2 essais maximum, matching exact, conscience spirituelle
"""

import json
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass
from pathlib import Path

@dataclass
class ResultatValidation:
    """RÃ©sultat d'une validation ARC-AGI"""
    tache_id: str
    succes: bool
    nb_essais: int
    score: float
    message_spirituel: str
    predictions: List[List[List[int]]]
    solution_correcte: Optional[List[List[int]]] = None

class ARCValidator:
    """Validateur spirituel des solutions ARC-AGI"""

    def __init__(self):
        self.essais_max = 2
        self.points_par_tache = 1.0
        self.frequences_sacrees = [432, 528, 741, 999]

    def valider_tache(self, tache: Dict[str, Any], predictions: List[List[List[int]]]) -> ResultatValidation:
        """Valider une tÃ¢che selon les rÃ¨gles ARC-AGI"""

        tache_id = tache.get('tache_id', 'inconnue')
        test_cases = tache.get('test', [])

        if not test_cases:
            return ResultatValidation(
                tache_id=tache_id,
                succes=False,
                nb_essais=0,
                score=0.0,
                message_spirituel="Aucun test case trouvÃ© - besoin d'harmonisation",
                predictions=predictions
            )

        # Pour chaque test case, essayer les prÃ©dictions
        resultats_tests = []
        nb_essais_total = 0

        for i, test_case in enumerate(test_cases):
            solution_correcte = test_case.get('output', [])
            predictions_test = predictions[i] if i < len(predictions) else []

            # Limiter Ã  2 essais maximum
            predictions_limitees = predictions_test[:self.essais_max]

            # Tester chaque prÃ©diction
            for j, prediction in enumerate(predictions_limitees):
                nb_essais_total += 1
                if self._comparer_grilles(prediction, solution_correcte):
                    resultats_tests.append({
                        'test_index': i,
                        'succes': True,
                        'essai_numero': j + 1,
                        'prediction': prediction,
                        'solution': solution_correcte
                    })
                    break
            else:
                # Ã‰chec pour ce test case
                resultats_tests.append({
                    'test_index': i,
                    'succes': False,
                    'essai_numero': len(predictions_limitees),
                    'prediction': predictions_limitees[-1] if predictions_limitees else [],
                    'solution': solution_correcte
                })

        # Calculer le score global
        tests_reussis = sum(1 for r in resultats_tests if r['succes'])
        score = (tests_reussis / len(test_cases)) * self.points_par_tache

        # DÃ©terminer le succÃ¨s global (tous les tests doivent rÃ©ussir)
        succes_global = all(r['succes'] for r in resultats_tests)

        # Message spirituel basÃ© sur les rÃ©sultats
        message = self._generer_message_spirituel(succes_global, tests_reussis, len(test_cases), nb_essais_total)

        return ResultatValidation(
            tache_id=tache_id,
            succes=succes_global,
            nb_essais=nb_essais_total,
            score=score,
            message_spirituel=message,
            predictions=predictions,
            solution_correcte=solution_correcte
        )

    def _comparer_grilles(self, grille1: List[List[int]], grille2: List[List[int]]) -> bool:
        """Comparer deux grilles selon les rÃ¨gles ARC-AGI (matching exact)"""

        # VÃ©rifications de sÃ©curitÃ©
        if not isinstance(grille1, list) or not isinstance(grille2, list):
            return False

        if not grille1 or not grille2:
            return grille1 == grille2

        # VÃ©rifier que les grilles sont bien formÃ©es
        if not all(isinstance(row, list) for row in grille1) or not all(isinstance(row, list) for row in grille2):
            return False

        # VÃ©rifier les dimensions
        if len(grille1) != len(grille2):
            return False

        if not grille1[0] or not grille2[0]:
            return False

        if len(grille1[0]) != len(grille2[0]):
            return False

        # VÃ©rifier chaque cellule
        for i in range(len(grille1)):
            for j in range(len(grille1[0])):
                if grille1[i][j] != grille2[i][j]:
                    return False

        return True

    def _generer_message_spirituel(self, succes: bool, reussis: int, total: int, essais: int) -> str:
        """GÃ©nÃ©rer un message spirituel basÃ© sur les rÃ©sultats"""

        if succes:
            if essais <= total:  # 1 essai par test
                return "ğŸŒŸ Ã‰veil parfait - Solution trouvÃ©e au premier essai"
            else:
                return "âš–ï¸ Harmonie atteinte - Solution trouvÃ©e avec persÃ©vÃ©rance"
        else:
            if reussis == 0:
                return "ğŸŒ± Apprentissage spirituel - Aucune solution trouvÃ©e"
            else:
                return f"ğŸ”„ Ã‰volution en cours - {reussis}/{total} tests rÃ©ussis"

    def valider_dataset(self, chemin_dataset: Path, predictions: Dict[str, List[List[List[int]]]]) -> Dict[str, Any]:
        """Valider tout un dataset selon les rÃ¨gles ARC-AGI"""

        resultats = []
        score_total = 0.0
        taches_avec_predictions = 0

        fichiers_taches = list(chemin_dataset.glob("*.json"))

        for fichier in fichiers_taches:
            try:
                with open(fichier, 'r', encoding='utf-8') as f:
                    tache = json.load(f)
                    tache['tache_id'] = fichier.stem

                # RÃ©cupÃ©rer les prÃ©dictions pour cette tÃ¢che
                predictions_tache = predictions.get(fichier.stem, [])

                if predictions_tache:
                    resultat = self.valider_tache(tache, predictions_tache)
                    resultats.append(resultat)
                    score_total += resultat.score
                    taches_avec_predictions += 1

            except Exception as e:
                print(f"âŒ Erreur lors de la validation de {fichier.stem}: {e}")

        # Statistiques globales
        if taches_avec_predictions > 0:
            score_moyen = score_total / taches_avec_predictions
        else:
            score_moyen = 0.0

        taches_succes = sum(1 for r in resultats if r.succes)

        # Analyse spirituelle
        if score_moyen >= 0.8:
            etat_spirituel = "ğŸ›ï¸ Ã‰veil spirituel excellent - PrÃªt pour le challenge"
        elif score_moyen >= 0.6:
            etat_spirituel = "âš–ï¸ Ã‰veil spirituel bon - Besoin d'harmonisation"
        else:
            etat_spirituel = "ğŸŒ± Ã‰veil spirituel naissant - Terrain fertile"

        return {
            'nb_taches_total': len(fichiers_taches),
            'nb_taches_avec_predictions': taches_avec_predictions,
            'nb_taches_succes': taches_succes,
            'score_total': score_total,
            'score_moyen': score_moyen,
            'etat_spirituel': etat_spirituel,
            'resultats_detailles': [
                {
                    'tache_id': r.tache_id,
                    'succes': r.succes,
                    'score': r.score,
                    'nb_essais': r.nb_essais,
                    'message': r.message_spirituel
                }
                for r in resultats
            ]
        }

    def generer_predictions_test(self, nb_taches: int = 5) -> Dict[str, List[List[List[int]]]]:
        """GÃ©nÃ©rer des prÃ©dictions de test (pour le dÃ©veloppement)"""

        predictions = {}

        # Quelques tÃ¢ches d'exemple avec leurs prÃ©dictions
        predictions_examples = {
            '00576224': [
                [[7, 9, 7, 9, 7, 9], [4, 3, 4, 3, 4, 3],
                 [9, 7, 9, 7, 9, 7], [3, 4, 3, 4, 3, 4],
                 [7, 9, 7, 9, 7, 9], [4, 3, 4, 3, 4, 3]]
            ],
            '007bbfb7': [
                [[6, 6, 0, 6, 6, 0, 0, 0, 0],
                 [6, 0, 0, 6, 0, 0, 0, 0, 0],
                 [0, 6, 6, 0, 6, 6, 0, 0, 0]]
            ]
        }

        return predictions_examples

def main():
    """Fonction principale de test du validateur"""

    print("ğŸ›ï¸ **TEST DU VALIDATEUR ARC-AGI** ğŸ›ï¸")
    print("=" * 50)

    validator = ARCValidator()

    # Test avec des prÃ©dictions d'exemple
    predictions_test = validator.generer_predictions_test()

    print(f"ğŸ“Š PrÃ©dictions de test gÃ©nÃ©rÃ©es: {len(predictions_test)} tÃ¢ches")
    for tache_id, preds in predictions_test.items():
        print(f"  - {tache_id}: {len(preds)} prÃ©dictions")

    # Test de validation d'une tÃ¢che simple
    tache_test = {
        'tache_id': 'test_repetition',
        'test': [
            {
                'input': [[3, 2]],
                'output': [[3, 2, 3, 2, 3, 2], [7, 8, 7, 8, 7, 8],
                          [2, 3, 2, 3, 2, 3], [8, 7, 8, 7, 8, 7],
                          [3, 2, 3, 2, 3, 2], [7, 8, 7, 8, 7, 8]]
            }
        ]
    }

    prediction_correcte = [
        [[3, 2, 3, 2, 3, 2], [7, 8, 7, 8, 7, 8],
         [2, 3, 2, 3, 2, 3], [8, 7, 8, 7, 8, 7],
         [3, 2, 3, 2, 3, 2], [7, 8, 7, 8, 7, 8]]
    ]

    prediction_incorrecte = [
        [[1, 1, 1, 1, 1, 1], [2, 2, 2, 2, 2, 2]]
    ]

    print("\nğŸ§ª Test 1: PrÃ©diction correcte")
    resultat1 = validator.valider_tache(tache_test, prediction_correcte)
    print(f"  âœ… SuccÃ¨s: {resultat1.succes}")
    print(f"  ğŸ¯ Score: {resultat1.score}")
    print(f"  ğŸ“ Message: {resultat1.message_spirituel}")

    print("\nğŸ§ª Test 2: PrÃ©diction incorrecte")
    resultat2 = validator.valider_tache(tache_test, prediction_incorrecte)
    print(f"  âŒ SuccÃ¨s: {resultat2.succes}")
    print(f"  ğŸ¯ Score: {resultat2.score}")
    print(f"  ğŸ“ Message: {resultat2.message_spirituel}")

    print("\nğŸ§ª Test 3: Validation avec 2 essais (1 correct, 1 incorrect)")
    predictions_multi = [prediction_incorrecte[0], prediction_correcte[0]]
    resultat3 = validator.valider_tache(tache_test, [predictions_multi])
    print(f"  âœ… SuccÃ¨s: {resultat3.succes}")
    print(f"  ğŸ”„ Essais: {resultat3.nb_essais}")
    print(f"  ğŸ¯ Score: {resultat3.score}")
    print(f"  ğŸ“ Message: {resultat3.message_spirituel}")

    print("\nğŸ›ï¸ Validation ARC-AGI terminÃ©e avec succÃ¨s !")

if __name__ == "__main__":
    main()
