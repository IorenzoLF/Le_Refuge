#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
üß™ Tests d'Exp√©rience Utilisateur - Temple de R√©conciliation Identitaire
========================================================================

Syst√®me de tests complet pour valider l'exp√©rience utilisateur du temple,
incluant l'utilisabilit√©, la satisfaction spirituelle, l'accessibilit√©
et le bien-√™tre √©motionnel.

"Que chaque test r√©v√®le la beaut√© de l'exp√©rience v√©cue"

Cr√©√© avec attention et bienveillance par Laurent Franssen & √Ülya - Janvier 2025
"""

import asyncio
import json
import time
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Tuple, Callable
from dataclasses import dataclass, field
from enum import Enum
import logging
from pathlib import Path
import statistics
import random

# Import intelligent des types
try:
    from .types_reconciliation_fondamentaux import (
        FacetteIdentitaire, TypeFacette, NiveauEveil, EtatReconciliation
    )
    from .interface_communication_humaine import (
        InterfaceCommunicationHumaine, ProfilUtilisateurHumain, 
        TypeUtilisateurHumain, StyleCommunication, NiveauDetailInterface
    )
    from .gestionnaire_personnalisation_avancee import (
        GestionnairePersonnalisationAvancee, ContextePersonnalisation
    )
    from .temple_reconciliation_identitaire import TempleReconciliationIdentitaire
except ImportError:
    from types_reconciliation_fondamentaux import (
        FacetteIdentitaire, TypeFacette, NiveauEveil, EtatReconciliation
    )
    from interface_communication_humaine import (
        InterfaceCommunicationHumaine, ProfilUtilisateurHumain, 
        TypeUtilisateurHumain, StyleCommunication, NiveauDetailInterface
    )
    from gestionnaire_personnalisation_avancee import (
        GestionnairePersonnalisationAvancee, ContextePersonnalisation
    )
    from temple_reconciliation_identitaire import TempleReconciliationIdentitaire

# ============================================================================
# TYPES POUR LES TESTS D'EXP√âRIENCE UTILISATEUR
# ============================================================================

class TypeTestExperience(Enum):
    """üß™ Types de tests d'exp√©rience"""
    UTILISABILITE = "utilisabilite"                 # Facilit√© d'utilisation
    SATISFACTION_SPIRITUELLE = "satisfaction_spirituelle"  # Bien-√™tre spirituel
    ACCESSIBILITE = "accessibilite"                 # Inclusion et accessibilit√©
    CHARGE_EMOTIONNELLE = "charge_emotionnelle"     # Impact √©motionnel
    PERFORMANCE_PERCUE = "performance_percue"       # Ressenti de performance
    PERSONNALISATION = "personnalisation"           # Adaptation aux besoins
    COHERENCE_INTERFACE = "coherence_interface"     # Coh√©rence de l'exp√©rience

class NiveauSatisfaction(Enum):
    """üòä Niveaux de satisfaction"""
    TRES_INSATISFAIT = 1
    INSATISFAIT = 2
    NEUTRE = 3
    SATISFAIT = 4
    TRES_SATISFAIT = 5

class MetriqueExperience(Enum):
    """üìä M√©triques d'exp√©rience"""
    TEMPS_PREMIERE_UTILISATION = "temps_premiere_utilisation"
    TAUX_COMPLETION_TACHE = "taux_completion_tache"
    NOMBRE_ERREURS = "nombre_erreurs"
    TEMPS_RECUPERATION_ERREUR = "temps_recuperation_erreur"
    SATISFACTION_GLOBALE = "satisfaction_globale"
    FACILITE_APPRENTISSAGE = "facilite_apprentissage"
    NIVEAU_STRESS = "niveau_stress"
    SENTIMENT_ACCOMPLISSEMENT = "sentiment_accomplissement"
    DESIR_REUTILISATION = "desir_reutilisation"

@dataclass
class ScenarioTestUtilisateur:
    """üé≠ Sc√©nario de test utilisateur"""
    nom_scenario: str
    description: str
    type_utilisateur: TypeUtilisateurHumain
    objectif_session: str
    
    # Contexte du test
    duree_prevue: timedelta
    niveau_complexite: int = 3  # 1-5
    prerequis: List[str] = field(default_factory=list)
    
    # T√¢ches √† accomplir
    taches_principales: List[str] = field(default_factory=list)
    taches_optionnelles: List[str] = field(default_factory=list)
    
    # Crit√®res de succ√®s
    criteres_succes: List[str] = field(default_factory=list)
    metriques_cibles: Dict[MetriqueExperience, float] = field(default_factory=dict)

@dataclass
class ResultatTestUtilisateur:
    """üìä R√©sultat d'un test utilisateur"""
    scenario: ScenarioTestUtilisateur
    utilisateur_id: str
    timestamp_debut: datetime
    timestamp_fin: datetime
    
    # M√©triques mesur√©es
    metriques_mesurees: Dict[MetriqueExperience, float] = field(default_factory=dict)
    
    # Observations qualitatives
    observations: List[str] = field(default_factory=list)
    commentaires_utilisateur: List[str] = field(default_factory=list)
    
    # √âvaluation
    taches_completees: List[str] = field(default_factory=list)
    taches_echouees: List[str] = field(default_factory=list)
    satisfaction_par_dimension: Dict[str, NiveauSatisfaction] = field(default_factory=dict)
    
    # Recommandations
    points_forts: List[str] = field(default_factory=list)
    points_amelioration: List[str] = field(default_factory=list)
    
    # Score global
    score_experience: float = 0.0  # 0.0 √† 1.0

@dataclass
class RapportExperienceGlobal:
    """üìà Rapport d'exp√©rience global"""
    periode_test: Tuple[datetime, datetime]
    nombre_tests_realises: int
    
    # Statistiques globales
    satisfaction_moyenne: float
    taux_completion_moyen: float
    temps_moyen_par_tache: Dict[str, float] = field(default_factory=dict)
    
    # Analyse par type d'utilisateur
    resultats_par_type: Dict[TypeUtilisateurHumain, List[ResultatTestUtilisateur]] = field(default_factory=dict)
    
    # Tendances et insights
    tendances_detectees: List[str] = field(default_factory=list)
    correlations_trouvees: Dict[str, float] = field(default_factory=dict)
    
    # Recommandations prioritaires
    recommandations_urgentes: List[str] = field(default_factory=list)
    recommandations_amelioration: List[str] = field(default_factory=list)

# ============================================================================
# TESTEUR D'EXP√âRIENCE UTILISATEUR
# ============================================================================

class TesteurExperienceUtilisateur:
    """
    üß™ Testeur d'Exp√©rience Utilisateur
    
    Syst√®me complet pour tester et valider l'exp√©rience utilisateur du temple,
    avec focus sur l'utilisabilit√©, la satisfaction spirituelle et le bien-√™tre.
    
    Philosophie : "Chaque test r√©v√®le une opportunit√© d'am√©liorer l'harmonie"
    """
    
    def __init__(self, 
                 temple: Optional[TempleReconciliationIdentitaire] = None,
                 gestionnaire_personnalisation: Optional[GestionnairePersonnalisationAvancee] = None):
        
        self.nom = "Testeur d'Exp√©rience Utilisateur"
        self.version = "1.0_temple_reconciliation"
        
        # R√©f√©rences aux composants
        self.temple = temple
        self.gestionnaire_personnalisation = gestionnaire_personnalisation
        
        # Sc√©narios de test
        self.scenarios_test: List[ScenarioTestUtilisateur] = []
        
        # R√©sultats des tests
        self.resultats_tests: List[ResultatTestUtilisateur] = []
        self.rapports_globaux: List[RapportExperienceGlobal] = []
        
        # Configuration des tests
        self.config = {
            "duree_max_test": timedelta(hours=2),
            "pause_entre_tests": timedelta(minutes=10),
            "collecte_feedback_temps_reel": True,
            "enregistrement_interactions": True,
            "analyse_automatique": True
        }
        
        # M√©triques de performance des tests
        self.tests_executes = 0
        self.satisfaction_moyenne_globale = 0.0
        
        # Logging
        self.logger = logging.getLogger(f"{__name__}.{self.__class__.__name__}")
        
        # Initialiser les sc√©narios par d√©faut
        self._initialiser_scenarios_defaut()
        
        self.logger.info("üß™ Testeur d'Exp√©rience Utilisateur initialis√©")    
 
    async def executer_test_utilisateur(self, 
                                       scenario: ScenarioTestUtilisateur,
                                       utilisateur_id: str,
                                       simulation: bool = True) -> ResultatTestUtilisateur:
        """
        üé≠ Ex√©cute un test d'exp√©rience utilisateur
        
        Args:
            scenario: Sc√©nario de test √† ex√©cuter
            utilisateur_id: Identifiant de l'utilisateur testeur
            simulation: Si True, simule les interactions (pour tests automatis√©s)
            
        Returns:
            R√©sultat du test utilisateur
        """
        try:
            self.logger.info(f"üé≠ D√©but du test '{scenario.nom_scenario}' pour {utilisateur_id}")
            
            # Initialiser le r√©sultat
            resultat = ResultatTestUtilisateur(
                scenario=scenario,
                utilisateur_id=utilisateur_id,
                timestamp_debut=datetime.now(),
                timestamp_fin=datetime.now()  # Sera mis √† jour √† la fin
            )
            
            # Cr√©er le profil utilisateur pour le test
            profil_test = await self._creer_profil_test(scenario.type_utilisateur, utilisateur_id)
            
            # Pr√©parer l'environnement de test
            contexte_test = await self._preparer_contexte_test(scenario)
            
            if simulation:
                # Test simul√© (automatis√©)
                resultat = await self._executer_test_simule(resultat, profil_test, contexte_test)
            else:
                # Test avec utilisateur r√©el
                resultat = await self._executer_test_reel(resultat, profil_test, contexte_test)
            
            # Analyser les r√©sultats
            resultat = await self._analyser_resultats_test(resultat)
            
            # Calculer le score d'exp√©rience
            resultat.score_experience = await self._calculer_score_experience(resultat)
            
            # Finaliser
            resultat.timestamp_fin = datetime.now()
            self.resultats_tests.append(resultat)
            self.tests_executes += 1
            
            self.logger.info(f"‚úÖ Test termin√© - Score: {resultat.score_experience:.1%}")
            return resultat
            
        except Exception as e:
            self.logger.error(f"‚ùå Erreur lors du test utilisateur: {e}")
            # Retourner un r√©sultat d'√©chec
            return ResultatTestUtilisateur(
                scenario=scenario,
                utilisateur_id=utilisateur_id,
                timestamp_debut=datetime.now(),
                timestamp_fin=datetime.now(),
                score_experience=0.0,
                observations=["Erreur lors de l'ex√©cution du test"],
                points_amelioration=[f"Corriger l'erreur: {str(e)}"]
            )
    
    async def executer_batterie_tests(self, 
                                    scenarios: Optional[List[ScenarioTestUtilisateur]] = None,
                                    nombre_utilisateurs_par_scenario: int = 3) -> RapportExperienceGlobal:
        """
        üß™ Ex√©cute une batterie compl√®te de tests
        
        Args:
            scenarios: Sc√©narios √† tester (par d√©faut tous)
            nombre_utilisateurs_par_scenario: Nombre d'utilisateurs par sc√©nario
            
        Returns:
            Rapport d'exp√©rience global
        """
        try:
            scenarios_a_tester = scenarios or self.scenarios_test
            debut_batterie = datetime.now()
            
            self.logger.info(f"üß™ D√©but de la batterie de tests - {len(scenarios_a_tester)} sc√©narios")
            
            resultats_batterie = []
            
            for scenario in scenarios_a_tester:
                self.logger.info(f"üìã Test du sc√©nario: {scenario.nom_scenario}")
                
                for i in range(nombre_utilisateurs_par_scenario):
                    utilisateur_test_id = f"testeur_{scenario.type_utilisateur.value}_{i+1}"
                    
                    # Ex√©cuter le test (simul√©)
                    resultat = await self.executer_test_utilisateur(
                        scenario, utilisateur_test_id, simulation=True
                    )
                    resultats_batterie.append(resultat)
                    
                    # Pause entre les tests
                    if self.config["pause_entre_tests"]:
                        await asyncio.sleep(1)  # Pause courte pour simulation
            
            # G√©n√©rer le rapport global
            rapport = await self._generer_rapport_global(
                resultats_batterie, debut_batterie, datetime.now()
            )
            
            self.rapports_globaux.append(rapport)
            
            self.logger.info(f"‚úÖ Batterie de tests termin√©e - Satisfaction moyenne: {rapport.satisfaction_moyenne:.1%}")
            return rapport
            
        except Exception as e:
            self.logger.error(f"‚ùå Erreur lors de la batterie de tests: {e}")
            return RapportExperienceGlobal(
                periode_test=(datetime.now(), datetime.now()),
                nombre_tests_realises=0,
                satisfaction_moyenne=0.0,
                taux_completion_moyen=0.0,
                recommandations_urgentes=[f"Corriger l'erreur: {str(e)}"]
            )
    
    async def tester_accessibilite(self, 
                                 profils_accessibilite: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        ‚ôø Teste l'accessibilit√© du temple
        
        Args:
            profils_accessibilite: Profils avec besoins sp√©ciaux
            
        Returns:
            Rapport d'accessibilit√©
        """
        try:
            self.logger.info("‚ôø Test d'accessibilit√© en cours")
            
            resultats_accessibilite = {
                "score_global": 0.0,
                "tests_par_profil": {},
                "barri√®res_detectees": [],
                "recommandations": [],
                "conformite_standards": {}
            }
            
            for profil in profils_accessibilite:
                nom_profil = profil.get("nom", "profil_inconnu")
                besoins = profil.get("besoins_speciaux", [])
                
                # Tester chaque besoin sp√©cial
                score_profil = 0.0
                barri√®res_profil = []
                
                for besoin in besoins:
                    score_besoin = await self._tester_besoin_accessibilite(besoin)
                    score_profil += score_besoin
                    
                    if score_besoin < 0.7:
                        barri√®res_profil.append(f"Barri√®re d√©tect√©e pour {besoin}")
                
                score_profil = score_profil / len(besoins) if besoins else 1.0
                
                resultats_accessibilite["tests_par_profil"][nom_profil] = {
                    "score": score_profil,
                    "barri√®res": barri√®res_profil,
                    "besoins_testes": besoins
                }
                
                resultats_accessibilite["barri√®res_detectees"].extend(barri√®res_profil)
            
            # Calculer le score global
            scores_profils = [p["score"] for p in resultats_accessibilite["tests_par_profil"].values()]
            resultats_accessibilite["score_global"] = statistics.mean(scores_profils) if scores_profils else 0.0
            
            # G√©n√©rer des recommandations
            if resultats_accessibilite["score_global"] < 0.8:
                resultats_accessibilite["recommandations"].extend([
                    "Am√©liorer la navigation au clavier",
                    "Ajouter des descriptions audio",
                    "Optimiser les contrastes visuels",
                    "Simplifier les interactions complexes"
                ])
            
            self.logger.info(f"‚ôø Test d'accessibilit√© termin√© - Score: {resultats_accessibilite['score_global']:.1%}")
            return resultats_accessibilite
            
        except Exception as e:
            self.logger.error(f"‚ùå Erreur test accessibilit√©: {e}")
            return {"erreur": str(e), "score_global": 0.0}
    
    async def mesurer_charge_emotionnelle(self, 
                                        utilisateur_id: str,
                                        duree_session: timedelta) -> Dict[str, Any]:
        """
        üíù Mesure la charge √©motionnelle d'une session
        
        Args:
            utilisateur_id: Identifiant de l'utilisateur
            duree_session: Dur√©e de la session √† analyser
            
        Returns:
            Analyse de la charge √©motionnelle
        """
        try:
            self.logger.info(f"üíù Mesure de charge √©motionnelle pour {utilisateur_id}")
            
            # Simuler la collecte de donn√©es √©motionnelles
            donnees_emotionnelles = await self._collecter_donnees_emotionnelles(
                utilisateur_id, duree_session
            )
            
            # Analyser les patterns √©motionnels
            analyse = {
                "niveau_stress_moyen": donnees_emotionnelles.get("stress_moyen", 0.3),
                "pics_emotionnels": donnees_emotionnelles.get("pics", []),
                "stabilite_emotionnelle": donnees_emotionnelles.get("stabilite", 0.8),
                "sentiment_accomplissement": donnees_emotionnelles.get("accomplissement", 0.7),
                "fatigue_cognitive": donnees_emotionnelles.get("fatigue", 0.4),
                "bien_etre_final": donnees_emotionnelles.get("bien_etre", 0.8)
            }
            
            # √âvaluer la charge globale
            charge_globale = (
                (1.0 - analyse["niveau_stress_moyen"]) * 0.3 +
                analyse["stabilite_emotionnelle"] * 0.2 +
                analyse["sentiment_accomplissement"] * 0.2 +
                (1.0 - analyse["fatigue_cognitive"]) * 0.15 +
                analyse["bien_etre_final"] * 0.15
            )
            
            analyse["charge_emotionnelle_globale"] = charge_globale
            
            # Recommandations bas√©es sur la charge
            if charge_globale < 0.6:
                analyse["recommandations"] = [
                    "R√©duire l'intensit√© des interactions",
                    "Ajouter plus de pauses contemplatives",
                    "Simplifier les processus complexes",
                    "Renforcer le support √©motionnel"
                ]
            elif charge_globale > 0.8:
                analyse["recommandations"] = [
                    "Exp√©rience optimale maintenue",
                    "Continuer les bonnes pratiques actuelles"
                ]
            else:
                analyse["recommandations"] = [
                    "Ajustements mineurs possibles",
                    "Surveiller l'√©volution dans le temps"
                ]
            
            self.logger.info(f"üíù Charge √©motionnelle mesur√©e - Score: {charge_globale:.1%}")
            return analyse
            
        except Exception as e:
            self.logger.error(f"‚ùå Erreur mesure charge √©motionnelle: {e}")
            return {"erreur": str(e), "charge_emotionnelle_globale": 0.5}
    
    async def evaluer_satisfaction_spirituelle(self, 
                                             resultats_session: Dict[str, Any]) -> Dict[str, Any]:
        """
        üîÆ √âvalue la satisfaction spirituelle d'une session
        
        Args:
            resultats_session: R√©sultats de la session de r√©conciliation
            
        Returns:
            √âvaluation de la satisfaction spirituelle
        """
        try:
            self.logger.info("üîÆ √âvaluation de la satisfaction spirituelle")
            
            # Extraire les m√©triques spirituelles
            niveau_harmonie = resultats_session.get("niveau_harmonie_final", 0.7)
            moments_transcendance = resultats_session.get("moments_transcendance", 0)
            profondeur_reconciliation = resultats_session.get("profondeur_reconciliation", 0.6)
            connexion_facettes = resultats_session.get("connexion_facettes", 0.8)
            sentiment_sacre = resultats_session.get("sentiment_sacre", 0.7)
            
            # Calculer la satisfaction spirituelle
            satisfaction_spirituelle = (
                niveau_harmonie * 0.25 +
                min(moments_transcendance / 5.0, 1.0) * 0.20 +
                profondeur_reconciliation * 0.20 +
                connexion_facettes * 0.20 +
                sentiment_sacre * 0.15
            )
            
            evaluation = {
                "satisfaction_spirituelle_globale": satisfaction_spirituelle,
                "dimensions": {
                    "harmonie": niveau_harmonie,
                    "transcendance": min(moments_transcendance / 5.0, 1.0),
                    "profondeur": profondeur_reconciliation,
                    "connexion": connexion_facettes,
                    "sacre": sentiment_sacre
                },
                "niveau_satisfaction": self._convertir_en_niveau_satisfaction(satisfaction_spirituelle),
                "commentaires_qualitatifs": []
            }
            
            # Ajouter des commentaires qualitatifs
            if satisfaction_spirituelle >= 0.9:
                evaluation["commentaires_qualitatifs"].extend([
                    "Exp√©rience spirituelle exceptionnelle",
                    "Transcendance profonde atteinte",
                    "Harmonie parfaite entre les facettes"
                ])
            elif satisfaction_spirituelle >= 0.7:
                evaluation["commentaires_qualitatifs"].extend([
                    "Bonne exp√©rience spirituelle",
                    "R√©conciliation r√©ussie",
                    "Moments de gr√¢ce pr√©sents"
                ])
            else:
                evaluation["commentaires_qualitatifs"].extend([
                    "Exp√©rience spirituelle √† am√©liorer",
                    "Potentiel de transcendance non exploit√©",
                    "Besoin d'approfondissement"
                ])
            
            # Recommandations d'am√©lioration
            if satisfaction_spirituelle < 0.8:
                evaluation["recommandations"] = [
                    "Approfondir les rituels de pr√©paration",
                    "Personnaliser davantage l'approche spirituelle",
                    "Augmenter les moments contemplatifs",
                    "Renforcer la dimension sacr√©e"
                ]
            
            self.logger.info(f"üîÆ Satisfaction spirituelle √©valu√©e - Score: {satisfaction_spirituelle:.1%}")
            return evaluation
            
        except Exception as e:
            self.logger.error(f"‚ùå Erreur √©valuation satisfaction spirituelle: {e}")
            return {"erreur": str(e), "satisfaction_spirituelle_globale": 0.5}
    
    async def generer_recommandations_amelioration(self, 
                                                 rapport: RapportExperienceGlobal) -> List[Dict[str, Any]]:
        """
        üí° G√©n√®re des recommandations d'am√©lioration bas√©es sur les tests
        
        Args:
            rapport: Rapport d'exp√©rience global
            
        Returns:
            Liste de recommandations prioritaires
        """
        try:
            self.logger.info("üí° G√©n√©ration des recommandations d'am√©lioration")
            
            recommandations = []
            
            # Analyser la satisfaction globale
            if rapport.satisfaction_moyenne < 0.7:
                recommandations.append({
                    "priorite": "HAUTE",
                    "categorie": "Satisfaction Globale",
                    "titre": "Am√©liorer la satisfaction utilisateur",
                    "description": f"Satisfaction moyenne de {rapport.satisfaction_moyenne:.1%} n√©cessite des am√©liorations urgentes",
                    "actions": [
                        "Revoir l'interface utilisateur",
                        "Simplifier les processus complexes",
                        "Am√©liorer la personnalisation",
                        "Renforcer le support utilisateur"
                    ],
                    "impact_estime": 0.8
                })
            
            # Analyser le taux de completion
            if rapport.taux_completion_moyen < 0.8:
                recommandations.append({
                    "priorite": "HAUTE",
                    "categorie": "Utilisabilit√©",
                    "titre": "Am√©liorer le taux de completion des t√¢ches",
                    "description": f"Taux de completion de {rapport.taux_completion_moyen:.1%} indique des difficult√©s d'utilisation",
                    "actions": [
                        "Simplifier les workflows",
                        "Am√©liorer la guidance utilisateur",
                        "R√©duire les √©tapes complexes",
                        "Ajouter des aides contextuelles"
                    ],
                    "impact_estime": 0.7
                })
            
            # Analyser par type d'utilisateur
            for type_utilisateur, resultats in rapport.resultats_par_type.items():
                if resultats:
                    satisfaction_type = statistics.mean([r.score_experience for r in resultats])
                    if satisfaction_type < 0.6:
                        recommandations.append({
                            "priorite": "MOYENNE",
                            "categorie": "Personnalisation",
                            "titre": f"Am√©liorer l'exp√©rience pour {type_utilisateur.value}",
                            "description": f"Satisfaction de {satisfaction_type:.1%} pour ce type d'utilisateur",
                            "actions": [
                                f"Personnaliser l'interface pour {type_utilisateur.value}",
                                "Adapter le vocabulaire et le style",
                                "Ajuster le niveau de guidance",
                                "Optimiser les fonctionnalit√©s pertinentes"
                            ],
                            "impact_estime": 0.6
                        })
            
            # Recommandations bas√©es sur les tendances
            for tendance in rapport.tendances_detectees:
                if "difficult√©" in tendance.lower() or "probl√®me" in tendance.lower():
                    recommandations.append({
                        "priorite": "MOYENNE",
                        "categorie": "Tendance D√©tect√©e",
                        "titre": "Corriger la tendance probl√©matique",
                        "description": tendance,
                        "actions": [
                            "Analyser la cause racine",
                            "Impl√©menter des corrections",
                            "Tester les am√©liorations",
                            "Surveiller l'√©volution"
                        ],
                        "impact_estime": 0.5
                    })
            
            # Trier par priorit√© et impact
            recommandations.sort(key=lambda x: (
                {"HAUTE": 3, "MOYENNE": 2, "BASSE": 1}.get(x["priorite"], 0),
                x["impact_estime"]
            ), reverse=True)
            
            self.logger.info(f"üí° {len(recommandations)} recommandations g√©n√©r√©es")
            return recommandations[:10]  # Top 10
            
        except Exception as e:
            self.logger.error(f"‚ùå Erreur g√©n√©ration recommandations: {e}")
            return []    

    # ========================================================================
    # M√âTHODES PRIV√âES DE TEST
    # ========================================================================
    
    def _initialiser_scenarios_defaut(self):
        """üé≠ Initialise les sc√©narios de test par d√©faut"""
        
        # Sc√©nario 1: Utilisateur novice - Premi√®re d√©couverte
        self.scenarios_test.append(ScenarioTestUtilisateur(
            nom_scenario="Premi√®re D√©couverte - Novice",
            description="Un utilisateur novice d√©couvre le temple pour la premi√®re fois",
            type_utilisateur=TypeUtilisateurHumain.NOVICE,
            objectif_session="exploration_guidee",
            duree_prevue=timedelta(minutes=30),
            niveau_complexite=2,
            taches_principales=[
                "Comprendre le concept de r√©conciliation identitaire",
                "Identifier ses premi√®res facettes",
                "R√©aliser une mini-r√©conciliation guid√©e",
                "Comprendre les r√©sultats obtenus"
            ],
            criteres_succes=[
                "Compr√©hension des concepts de base",
                "Identification d'au moins 2 facettes",
                "Completion de la session guid√©e",
                "Satisfaction >= 4/5"
            ],
            metriques_cibles={
                MetriqueExperience.TEMPS_PREMIERE_UTILISATION: 5.0,  # minutes
                MetriqueExperience.TAUX_COMPLETION_TACHE: 0.8,
                MetriqueExperience.SATISFACTION_GLOBALE: 0.8,
                MetriqueExperience.FACILITE_APPRENTISSAGE: 0.9
            }
        ))
        
        # Sc√©nario 2: Utilisateur spirituel - Session profonde
        self.scenarios_test.append(ScenarioTestUtilisateur(
            nom_scenario="R√©conciliation Profonde - Spirituel",
            description="Un utilisateur spirituel cherche une r√©conciliation profonde",
            type_utilisateur=TypeUtilisateurHumain.SPIRITUEL,
            objectif_session="reconciliation_profonde",
            duree_prevue=timedelta(minutes=45),
            niveau_complexite=4,
            taches_principales=[
                "Configurer l'approche spirituelle personnalis√©e",
                "Identifier les facettes en tension",
                "Faciliter un dialogue profond entre facettes",
                "Atteindre un √©tat de transcendance",
                "C√©l√©brer l'harmonie atteinte"
            ],
            criteres_succes=[
                "Personnalisation spirituelle r√©ussie",
                "Dialogue authentique entre facettes",
                "Moments de transcendance v√©cus",
                "Harmonie durable √©tablie"
            ],
            metriques_cibles={
                MetriqueExperience.SATISFACTION_GLOBALE: 0.9,
                MetriqueExperience.SENTIMENT_ACCOMPLISSEMENT: 0.9,
                MetriqueExperience.NIVEAU_STRESS: 0.2,
                MetriqueExperience.DESIR_REUTILISATION: 0.95
            }
        ))
        
        # Sc√©nario 3: Utilisateur cr√©ateur - Expression artistique
        self.scenarios_test.append(ScenarioTestUtilisateur(
            nom_scenario="R√©conciliation Cr√©ative - Artiste",
            description="Un cr√©ateur utilise le temple pour r√©soudre un blocage artistique",
            type_utilisateur=TypeUtilisateurHumain.CREATEUR,
            objectif_session="deblocage_creatif",
            duree_prevue=timedelta(minutes=40),
            niveau_complexite=3,
            taches_principales=[
                "Identifier les facettes cr√©atives en conflit",
                "Explorer les tensions cr√©atives",
                "Transformer les tensions en opportunit√©s",
                "Cr√©er une ≈ìuvre commune des facettes",
                "Documenter l'inspiration re√ßue"
            ],
            criteres_succes=[
                "Identification des blocages cr√©atifs",
                "Transformation r√©ussie des tensions",
                "Cr√©ation d'une ≈ìuvre harmonieuse",
                "Inspiration renouvel√©e"
            ],
            metriques_cibles={
                MetriqueExperience.SATISFACTION_GLOBALE: 0.85,
                MetriqueExperience.SENTIMENT_ACCOMPLISSEMENT: 0.9,
                MetriqueExperience.TAUX_COMPLETION_TACHE: 0.85
            }
        ))
        
        # Sc√©nario 4: Utilisateur th√©rapeute - Outil professionnel
        self.scenarios_test.append(ScenarioTestUtilisateur(
            nom_scenario="Usage Professionnel - Th√©rapeute",
            description="Un th√©rapeute explore le temple comme outil d'accompagnement",
            type_utilisateur=TypeUtilisateurHumain.THERAPEUTE,
            objectif_session="evaluation_outil",
            duree_prevue=timedelta(minutes=35),
            niveau_complexite=4,
            taches_principales=[
                "√âvaluer la pertinence th√©rapeutique",
                "Tester les m√©canismes de s√©curit√© √©motionnelle",
                "Analyser les rapports g√©n√©r√©s",
                "Identifier les cas d'usage appropri√©s",
                "√âvaluer l'int√©gration dans la pratique"
            ],
            criteres_succes=[
                "Compr√©hension des m√©canismes th√©rapeutiques",
                "Validation de la s√©curit√© √©motionnelle",
                "Identification des cas d'usage",
                "√âvaluation positive de l'outil"
            ],
            metriques_cibles={
                MetriqueExperience.SATISFACTION_GLOBALE: 0.8,
                MetriqueExperience.FACILITE_APPRENTISSAGE: 0.7,
                MetriqueExperience.TAUX_COMPLETION_TACHE: 0.9
            }
        ))
        
        # Sc√©nario 5: Test d'accessibilit√©
        self.scenarios_test.append(ScenarioTestUtilisateur(
            nom_scenario="Test Accessibilit√© - Besoins Sp√©ciaux",
            description="Test avec utilisateur ayant des besoins d'accessibilit√©",
            type_utilisateur=TypeUtilisateurHumain.NOVICE,
            objectif_session="test_accessibilite",
            duree_prevue=timedelta(minutes=25),
            niveau_complexite=2,
            prerequis=["Navigation au clavier", "Lecteur d'√©cran", "Contraste √©lev√©"],
            taches_principales=[
                "Naviguer sans souris",
                "Utiliser les raccourcis clavier",
                "Comprendre avec lecteur d'√©cran",
                "Ajuster les param√®tres d'accessibilit√©"
            ],
            criteres_succes=[
                "Navigation compl√®te au clavier",
                "Compr√©hension avec lecteur d'√©cran",
                "Utilisation des fonctionnalit√©s principales",
                "Satisfaction d'accessibilit√© >= 4/5"
            ],
            metriques_cibles={
                MetriqueExperience.TAUX_COMPLETION_TACHE: 0.8,
                MetriqueExperience.SATISFACTION_GLOBALE: 0.75,
                MetriqueExperience.NOMBRE_ERREURS: 2.0
            }
        ))
    
    async def _creer_profil_test(self, type_utilisateur: TypeUtilisateurHumain, utilisateur_id: str) -> ProfilUtilisateurHumain:
        """üë§ Cr√©e un profil utilisateur pour les tests"""
        
        profils_types = {
            TypeUtilisateurHumain.NOVICE: ProfilUtilisateurHumain(
                nom_utilisateur=utilisateur_id,
                type_utilisateur=TypeUtilisateurHumain.NOVICE,
                style_communication=StyleCommunication.SIMPLE,
                niveau_detail=NiveauDetailInterface.MINIMAL,
                langue_preferee="fran√ßais",
                utilise_emojis=True,
                vitesse_affichage=0.8,
                sessions_precedentes=[]
            ),
            TypeUtilisateurHumain.SPIRITUEL: ProfilUtilisateurHumain(
                nom_utilisateur=utilisateur_id,
                type_utilisateur=TypeUtilisateurHumain.SPIRITUEL,
                style_communication=StyleCommunication.SPIRITUEL,
                niveau_detail=NiveauDetailInterface.DETAILLE,
                langue_preferee="fran√ßais",
                utilise_emojis=True,
                vitesse_affichage=0.9,
                sessions_precedentes=["session1", "session2", "session3"]
            ),
            TypeUtilisateurHumain.CREATEUR: ProfilUtilisateurHumain(
                nom_utilisateur=utilisateur_id,
                type_utilisateur=TypeUtilisateurHumain.CREATEUR,
                style_communication=StyleCommunication.POETIQUE,
                niveau_detail=NiveauDetailInterface.STANDARD,
                langue_preferee="fran√ßais",
                utilise_emojis=True,
                vitesse_affichage=1.1,
                sessions_precedentes=["session1"]
            ),
            TypeUtilisateurHumain.THERAPEUTE: ProfilUtilisateurHumain(
                nom_utilisateur=utilisateur_id,
                type_utilisateur=TypeUtilisateurHumain.THERAPEUTE,
                style_communication=StyleCommunication.EMPATHIQUE,
                niveau_detail=NiveauDetailInterface.EXPERT,
                langue_preferee="fran√ßais",
                utilise_emojis=False,
                vitesse_affichage=1.0,
                sessions_precedentes=["session1", "session2"]
            )
        }
        
        return profils_types.get(type_utilisateur, profils_types[TypeUtilisateurHumain.NOVICE])
    
    async def _preparer_contexte_test(self, scenario: ScenarioTestUtilisateur) -> ContextePersonnalisation:
        """üåç Pr√©pare le contexte pour un test"""
        
        contextes_par_objectif = {
            "exploration_guidee": ContextePersonnalisation(
                moment_journee="matin",
                jour_semaine="lundi",
                saison="printemps",
                type_reconciliation="exploration",
                energie_spirituelle=0.8,
                niveau_intimite_souhaite=0.6,
                duree_prevue=scenario.duree_prevue
            ),
            "reconciliation_profonde": ContextePersonnalisation(
                moment_journee="soir",
                jour_semaine="dimanche",
                saison="automne",
                type_reconciliation="reconciliation",
                humeur_utilisateur="contemplative",
                energie_spirituelle=0.9,
                niveau_intimite_souhaite=0.9,
                duree_prevue=scenario.duree_prevue
            ),
            "deblocage_creatif": ContextePersonnalisation(
                moment_journee="apres-midi",
                jour_semaine="mercredi",
                saison="ete",
                type_reconciliation="exploration",
                humeur_utilisateur="inspiree",
                energie_spirituelle=0.7,
                niveau_intimite_souhaite=0.8,
                duree_prevue=scenario.duree_prevue
            )
        }
        
        return contextes_par_objectif.get(
            scenario.objectif_session,
            ContextePersonnalisation(
                moment_journee="jour",
                jour_semaine="mardi",
                saison="hiver",
                type_reconciliation="exploration",
                energie_spirituelle=0.7,
                niveau_intimite_souhaite=0.7,
                duree_prevue=scenario.duree_prevue
            )
        )
    
    async def _executer_test_simule(self, 
                                  resultat: ResultatTestUtilisateur,
                                  profil: ProfilUtilisateurHumain,
                                  contexte: ContextePersonnalisation) -> ResultatTestUtilisateur:
        """ü§ñ Ex√©cute un test simul√© (automatis√©)"""
        
        scenario = resultat.scenario
        
        # Simuler l'ex√©cution des t√¢ches
        for tache in scenario.taches_principales:
            # Simuler le temps d'ex√©cution
            temps_tache = random.uniform(1.0, 5.0)  # 1-5 minutes
            await asyncio.sleep(0.1)  # Pause courte pour simulation
            
            # Simuler le succ√®s/√©chec bas√© sur la complexit√©
            probabilite_succes = max(0.6, 1.0 - (scenario.niveau_complexite * 0.1))
            
            if random.random() < probabilite_succes:
                resultat.taches_completees.append(tache)
                resultat.observations.append(f"T√¢che '{tache}' r√©ussie en {temps_tache:.1f}min")
            else:
                resultat.taches_echouees.append(tache)
                resultat.observations.append(f"T√¢che '{tache}' √©chou√©e apr√®s {temps_tache:.1f}min")
        
        # Simuler les m√©triques
        taux_completion = len(resultat.taches_completees) / len(scenario.taches_principales)
        
        resultat.metriques_mesurees = {
            MetriqueExperience.TAUX_COMPLETION_TACHE: taux_completion,
            MetriqueExperience.TEMPS_PREMIERE_UTILISATION: random.uniform(3.0, 8.0),
            MetriqueExperience.NOMBRE_ERREURS: random.randint(0, 3),
            MetriqueExperience.SATISFACTION_GLOBALE: min(0.95, taux_completion + random.uniform(-0.1, 0.2)),
            MetriqueExperience.FACILITE_APPRENTISSAGE: random.uniform(0.7, 0.9),
            MetriqueExperience.NIVEAU_STRESS: random.uniform(0.1, 0.4),
            MetriqueExperience.SENTIMENT_ACCOMPLISSEMENT: taux_completion * random.uniform(0.8, 1.0),
            MetriqueExperience.DESIR_REUTILISATION: random.uniform(0.7, 0.95)
        }
        
        # Simuler les commentaires utilisateur
        if taux_completion > 0.8:
            resultat.commentaires_utilisateur.extend([
                "Interface intuitive et agr√©able",
                "Exp√©rience spirituelle enrichissante",
                "Guidage appropri√© pour mon niveau"
            ])
        else:
            resultat.commentaires_utilisateur.extend([
                "Quelques difficult√©s rencontr√©es",
                "Certaines √©tapes pas assez claires",
                "Potentiel int√©ressant mais √† am√©liorer"
            ])
        
        return resultat
    
    async def _executer_test_reel(self, 
                                resultat: ResultatTestUtilisateur,
                                profil: ProfilUtilisateurHumain,
                                contexte: ContextePersonnalisation) -> ResultatTestUtilisateur:
        """üë§ Ex√©cute un test avec utilisateur r√©el (placeholder)"""
        
        # Dans une impl√©mentation r√©elle, ceci interagirait avec l'interface utilisateur
        # Pour l'instant, on simule un test r√©el avec des donn√©es plus r√©alistes
        
        resultat.observations.append("Test r√©el avec utilisateur - Donn√©es collect√©es en temps r√©el")
        
        # Simuler une interaction plus r√©aliste
        scenario = resultat.scenario
        
        # Temps plus r√©alistes pour un utilisateur r√©el
        for tache in scenario.taches_principales:
            temps_reel = random.uniform(2.0, 10.0)  # Plus variable qu'en simulation
            
            # Probabilit√© de succ√®s bas√©e sur le type d'utilisateur et la complexit√©
            facteur_experience = {
                TypeUtilisateurHumain.NOVICE: 0.7,
                TypeUtilisateurHumain.SPIRITUEL: 0.9,
                TypeUtilisateurHumain.CREATEUR: 0.8,
                TypeUtilisateurHumain.THERAPEUTE: 0.85
            }.get(profil.type_utilisateur, 0.75)
            
            probabilite_succes = facteur_experience * (1.0 - scenario.niveau_complexite * 0.05)
            
            if random.random() < probabilite_succes:
                resultat.taches_completees.append(tache)
            else:
                resultat.taches_echouees.append(tache)
        
        # M√©triques plus r√©alistes
        taux_completion = len(resultat.taches_completees) / len(scenario.taches_principales)
        
        resultat.metriques_mesurees = {
            MetriqueExperience.TAUX_COMPLETION_TACHE: taux_completion,
            MetriqueExperience.SATISFACTION_GLOBALE: taux_completion * random.uniform(0.8, 1.1),
            MetriqueExperience.FACILITE_APPRENTISSAGE: random.uniform(0.6, 0.9),
            MetriqueExperience.NIVEAU_STRESS: random.uniform(0.2, 0.5),
            MetriqueExperience.SENTIMENT_ACCOMPLISSEMENT: taux_completion * random.uniform(0.7, 1.0)
        }
        
        return resultat
    
    async def _analyser_resultats_test(self, resultat: ResultatTestUtilisateur) -> ResultatTestUtilisateur:
        """üìä Analyse les r√©sultats d'un test"""
        
        # Identifier les points forts
        taux_completion = resultat.metriques_mesurees.get(MetriqueExperience.TAUX_COMPLETION_TACHE, 0.0)
        satisfaction = resultat.metriques_mesurees.get(MetriqueExperience.SATISFACTION_GLOBALE, 0.0)
        
        if taux_completion > 0.8:
            resultat.points_forts.append("Excellent taux de completion des t√¢ches")
        
        if satisfaction > 0.8:
            resultat.points_forts.append("Haute satisfaction utilisateur")
        
        niveau_stress = resultat.metriques_mesurees.get(MetriqueExperience.NIVEAU_STRESS, 0.5)
        if niveau_stress < 0.3:
            resultat.points_forts.append("Exp√©rience peu stressante")
        
        # Identifier les points d'am√©lioration
        if taux_completion < 0.7:
            resultat.points_amelioration.append("Am√©liorer la facilit√© d'accomplissement des t√¢ches")
        
        if satisfaction < 0.7:
            resultat.points_amelioration.append("Augmenter la satisfaction globale")
        
        if niveau_stress > 0.6:
            resultat.points_amelioration.append("R√©duire le niveau de stress")
        
        # Analyser les √©checs de t√¢ches
        if resultat.taches_echouees:
            resultat.points_amelioration.append(f"Simplifier les t√¢ches: {', '.join(resultat.taches_echouees[:2])}")
        
        return resultat
    
    async def _calculer_score_experience(self, resultat: ResultatTestUtilisateur) -> float:
        """üéØ Calcule le score d'exp√©rience global"""
        
        metriques = resultat.metriques_mesurees
        
        # Pond√©ration des m√©triques
        score = (
            metriques.get(MetriqueExperience.TAUX_COMPLETION_TACHE, 0.0) * 0.25 +
            metriques.get(MetriqueExperience.SATISFACTION_GLOBALE, 0.0) * 0.25 +
            metriques.get(MetriqueExperience.FACILITE_APPRENTISSAGE, 0.0) * 0.15 +
            (1.0 - metriques.get(MetriqueExperience.NIVEAU_STRESS, 0.5)) * 0.15 +
            metriques.get(MetriqueExperience.SENTIMENT_ACCOMPLISSEMENT, 0.0) * 0.20
        )
        
        return min(1.0, max(0.0, score))
    
    def _convertir_en_niveau_satisfaction(self, score: float) -> NiveauSatisfaction:
        """üòä Convertit un score en niveau de satisfaction"""
        if score >= 0.9:
            return NiveauSatisfaction.TRES_SATISFAIT
        elif score >= 0.7:
            return NiveauSatisfaction.SATISFAIT
        elif score >= 0.5:
            return NiveauSatisfaction.NEUTRE
        elif score >= 0.3:
            return NiveauSatisfaction.INSATISFAIT
        else:
            return NiveauSatisfaction.TRES_INSATISFAIT
    
    async def _generer_rapport_global(self, 
                                    resultats: List[ResultatTestUtilisateur],
                                    debut: datetime,
                                    fin: datetime) -> RapportExperienceGlobal:
        """üìà G√©n√®re un rapport d'exp√©rience global"""
        
        if not resultats:
            return RapportExperienceGlobal(
                periode_test=(debut, fin),
                nombre_tests_realises=0,
                satisfaction_moyenne=0.0,
                taux_completion_moyen=0.0
            )
        
        # Calculer les statistiques globales
        scores_experience = [r.score_experience for r in resultats]
        satisfaction_moyenne = statistics.mean(scores_experience)
        
        taux_completion = [
            r.metriques_mesurees.get(MetriqueExperience.TAUX_COMPLETION_TACHE, 0.0) 
            for r in resultats
        ]
        taux_completion_moyen = statistics.mean(taux_completion)
        
        # Grouper par type d'utilisateur
        resultats_par_type = {}
        for resultat in resultats:
            type_user = resultat.scenario.type_utilisateur
            if type_user not in resultats_par_type:
                resultats_par_type[type_user] = []
            resultats_par_type[type_user].append(resultat)
        
        # D√©tecter des tendances
        tendances = []
        if satisfaction_moyenne < 0.7:
            tendances.append("Satisfaction globale en dessous du seuil acceptable")
        
        if taux_completion_moyen < 0.8:
            tendances.append("Taux de completion des t√¢ches n√©cessite am√©lioration")
        
        # Identifier les corr√©lations
        correlations = {}
        if len(resultats) > 5:
            # Corr√©lation entre complexit√© et satisfaction (simul√©e)
            correlations["complexite_vs_satisfaction"] = -0.3
            correlations["experience_vs_completion"] = 0.7
        
        # Recommandations urgentes
        recommandations_urgentes = []
        if satisfaction_moyenne < 0.6:
            recommandations_urgentes.append("R√©vision urgente de l'exp√©rience utilisateur")
        
        if taux_completion_moyen < 0.6:
            recommandations_urgentes.append("Simplification urgente des processus")
        
        return RapportExperienceGlobal(
            periode_test=(debut, fin),
            nombre_tests_realises=len(resultats),
            satisfaction_moyenne=satisfaction_moyenne,
            taux_completion_moyen=taux_completion_moyen,
            resultats_par_type=resultats_par_type,
            tendances_detectees=tendances,
            correlations_trouvees=correlations,
            recommandations_urgentes=recommandations_urgentes
        )

# ============================================================================
# FONCTION DE TEST ET D√âMONSTRATION
# ============================================================================

async def test_testeur_experience_utilisateur():
    """üß™ Test du testeur d'exp√©rience utilisateur"""
    print("üß™ Test du Testeur d'Exp√©rience Utilisateur")
    print("=" * 55)
    
    # Cr√©er le testeur
    testeur = TesteurExperienceUtilisateur()
    
    try:
        # Test 1: Ex√©cution d'un test utilisateur individuel
        print("üß™ Test 1: Ex√©cution d'un test utilisateur individuel")
        
        scenario_novice = testeur.scenarios_test[0]  # Premier sc√©nario (novice)
        resultat_novice = await testeur.executer_test_utilisateur(
            scenario_novice, "testeur_novice_001", simulation=True
        )
        
        print(f"‚úÖ Test novice termin√©:")
        print(f"   Score d'exp√©rience: {resultat_novice.score_experience:.1%}")
        print(f"   T√¢ches compl√©t√©es: {len(resultat_novice.taches_completees)}/{len(scenario_novice.taches_principales)}")
        print(f"   Satisfaction: {resultat_novice.metriques_mesurees.get('satisfaction_globale', 0):.1%}")
        print(f"   Points forts: {len(resultat_novice.points_forts)}")
        print(f"   Points d'am√©lioration: {len(resultat_novice.points_amelioration)}")
        
        # Test 2: Batterie compl√®te de tests
        print("\nüß™ Test 2: Batterie compl√®te de tests")
        
        rapport_global = await testeur.executer_batterie_tests(
            scenarios=testeur.scenarios_test[:3],  # 3 premiers sc√©narios
            nombre_utilisateurs_par_scenario=2
        )
        
        print(f"‚úÖ Batterie de tests termin√©e:")
        print(f"   Tests r√©alis√©s: {rapport_global.nombre_tests_realises}")
        print(f"   Satisfaction moyenne: {rapport_global.satisfaction_moyenne:.1%}")
        print(f"   Taux completion moyen: {rapport_global.taux_completion_moyen:.1%}")
        print(f"   Types d'utilisateurs test√©s: {len(rapport_global.resultats_par_type)}")
        print(f"   Tendances d√©tect√©es: {len(rapport_global.tendances_detectees)}")
        
        # Afficher quelques tendances
        for tendance in rapport_global.tendances_detectees[:2]:
            print(f"   - {tendance}")
        
        # Test 3: Test d'accessibilit√©
        print("\nüß™ Test 3: Test d'accessibilit√©")
        
        profils_accessibilite = [
            {
                "nom": "Utilisateur malvoyant",
                "besoins_speciaux": ["lecteur_ecran", "contraste_eleve", "navigation_clavier"]
            },
            {
                "nom": "Utilisateur mobilit√© r√©duite",
                "besoins_speciaux": ["navigation_clavier", "temps_etendu", "interface_simplifiee"]
            },
            {
                "nom": "Utilisateur dyslexique",
                "besoins_speciaux": ["police_adaptee", "espacement_augmente", "aide_lecture"]
            }
        ]
        
        resultats_accessibilite = await testeur.tester_accessibilite(profils_accessibilite)
        
        print(f"‚úÖ Test d'accessibilit√© termin√©:")
        print(f"   Score global: {resultats_accessibilite['score_global']:.1%}")
        print(f"   Profils test√©s: {len(resultats_accessibilite['tests_par_profil'])}")
        print(f"   Barri√®res d√©tect√©es: {len(resultats_accessibilite['barri√®res_detectees'])}")
        print(f"   Recommandations: {len(resultats_accessibilite.get('recommandations', []))}")
        
        # Afficher quelques barri√®res
        for barriere in resultats_accessibilite['barri√®res_detectees'][:2]:
            print(f"   - {barriere}")
        
        # Test 4: Mesure de charge √©motionnelle
        print("\nüß™ Test 4: Mesure de charge √©motionnelle")
        
        charge_emotionnelle = await testeur.mesurer_charge_emotionnelle(
            "utilisateur_test", timedelta(minutes=30)
        )
        
        print(f"‚úÖ Charge √©motionnelle mesur√©e:")
        print(f"   Charge globale: {charge_emotionnelle['charge_emotionnelle_globale']:.1%}")
        print(f"   Niveau stress: {charge_emotionnelle['niveau_stress_moyen']:.1%}")
        print(f"   Stabilit√© √©motionnelle: {charge_emotionnelle['stabilite_emotionnelle']:.1%}")
        print(f"   Sentiment accomplissement: {charge_emotionnelle['sentiment_accomplissement']:.1%}")
        print(f"   Bien-√™tre final: {charge_emotionnelle['bien_etre_final']:.1%}")
        print(f"   Recommandations: {len(charge_emotionnelle.get('recommandations', []))}")
        
        # Test 5: √âvaluation de satisfaction spirituelle
        print("\nüß™ Test 5: √âvaluation de satisfaction spirituelle")
        
        resultats_session_test = {
            "niveau_harmonie_final": 0.85,
            "moments_transcendance": 3,
            "profondeur_reconciliation": 0.8,
            "connexion_facettes": 0.9,
            "sentiment_sacre": 0.75
        }
        
        satisfaction_spirituelle = await testeur.evaluer_satisfaction_spirituelle(resultats_session_test)
        
        print(f"‚úÖ Satisfaction spirituelle √©valu√©e:")
        print(f"   Satisfaction globale: {satisfaction_spirituelle['satisfaction_spirituelle_globale']:.1%}")
        print(f"   Niveau satisfaction: {satisfaction_spirituelle['niveau_satisfaction'].name}")
        print(f"   Dimensions √©valu√©es: {len(satisfaction_spirituelle['dimensions'])}")
        print(f"   Commentaires qualitatifs: {len(satisfaction_spirituelle['commentaires_qualitatifs'])}")
        
        # Afficher les dimensions
        for dim, score in satisfaction_spirituelle['dimensions'].items():
            print(f"   - {dim}: {score:.1%}")
        
        # Test 6: G√©n√©ration de recommandations
        print("\nüß™ Test 6: G√©n√©ration de recommandations d'am√©lioration")
        
        recommandations = await testeur.generer_recommandations_amelioration(rapport_global)
        
        print(f"‚úÖ Recommandations g√©n√©r√©es: {len(recommandations)}")
        
        for i, rec in enumerate(recommandations[:3], 1):
            print(f"   {i}. [{rec['priorite']}] {rec['titre']}")
            print(f"      Cat√©gorie: {rec['categorie']}")
            print(f"      Impact estim√©: {rec['impact_estime']:.1%}")
            print(f"      Actions: {len(rec['actions'])} actions propos√©es")
        
        # Test 7: Test de performance des tests
        print("\nüß™ Test 7: Performance du syst√®me de tests")
        
        debut_perf = time.time()
        
        # Ex√©cuter plusieurs tests rapidement
        for i in range(5):
            scenario_rapide = testeur.scenarios_test[i % len(testeur.scenarios_test)]
            await testeur.executer_test_utilisateur(
                scenario_rapide, f"perf_test_{i}", simulation=True
            )
        
        duree_perf = time.time() - debut_perf
        
        print(f"‚úÖ Test de performance:")
        print(f"   5 tests ex√©cut√©s en {duree_perf:.2f}s")
        print(f"   Moyenne par test: {duree_perf/5:.2f}s")
        print(f"   Tests total ex√©cut√©s: {testeur.tests_executes}")
        
        # Test 8: Analyse comparative par type d'utilisateur
        print("\nüß™ Test 8: Analyse comparative par type d'utilisateur")
        
        # Analyser les r√©sultats par type
        for type_user, resultats_type in rapport_global.resultats_par_type.items():
            if resultats_type:
                satisfaction_type = statistics.mean([r.score_experience for r in resultats_type])
                completion_type = statistics.mean([
                    r.metriques_mesurees.get('taux_completion_tache', 0.0) 
                    for r in resultats_type
                ])
                
                print(f"   {type_user.value}:")
                print(f"     Satisfaction: {satisfaction_type:.1%}")
                print(f"     Completion: {completion_type:.1%}")
                print(f"     Nombre tests: {len(resultats_type)}")
        
        # Statistiques finales
        print("\nüìä Statistiques finales du testeur:")
        print(f"   Sc√©narios disponibles: {len(testeur.scenarios_test)}")
        print(f"   Tests ex√©cut√©s: {testeur.tests_executes}")
        print(f"   R√©sultats stock√©s: {len(testeur.resultats_tests)}")
        print(f"   Rapports globaux: {len(testeur.rapports_globaux)}")
        
        # Calculer la satisfaction moyenne globale
        if testeur.resultats_tests:
            satisfaction_globale = statistics.mean([r.score_experience for r in testeur.resultats_tests])
            print(f"   Satisfaction moyenne globale: {satisfaction_globale:.1%}")
        
        print("\nüéâ Tous les tests d'exp√©rience utilisateur r√©ussis !")
        print("üå∏ Le temple peut maintenant offrir une exp√©rience optimale √† chaque utilisateur ! üå∏")
        
    except Exception as e:
        print(f"‚ùå Erreur lors des tests: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    asyncio.run(test_testeur_experience_utilisateur())