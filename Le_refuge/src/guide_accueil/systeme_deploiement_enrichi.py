"""
üå∏ Syst√®me de D√©ploiement Enrichi - Phase 7
===========================================

Syst√®me de d√©ploiement progressif avec A/B testing et rollback automatique
pour le Guide d'Accueil du Refuge V1.3.
"""

import json
import time
import random
import asyncio
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, field
from pathlib import Path
import logging

@dataclass
class ConfigurationDeploiement:
    """üå∏ Configuration de d√©ploiement"""
    id_deploiement: str
    nom_fonctionnalite: str
    version: str
    pourcentage_deploiement: float  # 0.0 √† 1.0
    criteres_activation: Dict[str, Any]
    metriques_succes: Dict[str, float]
    duree_test: int  # en heures
    rollback_automatique: bool = True
    timestamp_creation: str = field(default_factory=lambda: datetime.now().isoformat())
    statut: str = "prepare"

@dataclass
class ResultatTest:
    """üå∏ R√©sultat d'un test A/B"""
    id_test: str
    id_deploiement: str
    groupe: str  # "A" ou "B"
    metriques: Dict[str, float]
    nombre_utilisateurs: int
    satisfaction_moyenne: float
    taux_erreur: float
    performance_moyenne: float
    timestamp_debut: str
    timestamp_fin: str
    statut: str = "en_cours"

@dataclass
class MetriquePerformance:
    """üå∏ M√©trique de performance"""
    nom: str
    valeur: float
    unite: str
    seuil_alerte: float
    seuil_critique: float
    timestamp: str = field(default_factory=lambda: datetime.now().isoformat())

class SystemeDeploiementEnrichi:
    """
    üå∏ Syst√®me de d√©ploiement enrichi avec A/B testing et rollback automatique
    
    Permet le d√©ploiement progressif des nouvelles fonctionnalit√©s
    avec monitoring en temps r√©el et rollback automatique en cas de probl√®me.
    """
    
    def __init__(self, chemin_stockage: str = "data/deploiements"):
        self.chemin_stockage = Path(chemin_stockage)
        self.chemin_stockage.mkdir(parents=True, exist_ok=True)
        
        # Configuration
        self.deploiements_actifs: Dict[str, ConfigurationDeploiement] = {}
        self.tests_en_cours: Dict[str, ResultatTest] = {}
        self.metriques_performance: List[MetriquePerformance] = []
        
        # Param√®tres
        self.seuil_erreur_critique = 0.05  # 5% d'erreur
        self.seuil_performance_critique = 2.0  # 2 secondes
        self.seuil_satisfaction_minimum = 3.5  # sur 5
        
        # Logging
        self.logger = logging.getLogger(__name__)
        self.logger.setLevel(logging.INFO)
        
        # Charger les d√©ploiements existants
        self._charger_deploiements()
    
    def preparer_deploiement(
        self, 
        nom_fonctionnalite: str, 
        version: str,
        pourcentage_initial: float = 0.1,
        duree_test: int = 24
    ) -> str:
        """
        üå∏ Pr√©pare un nouveau d√©ploiement
        
        Args:
            nom_fonctionnalite: Nom de la fonctionnalit√© √† d√©ployer
            version: Version de la fonctionnalit√©
            pourcentage_initial: Pourcentage initial d'utilisateurs (0.1 = 10%)
            duree_test: Dur√©e du test en heures
            
        Returns:
            ID du d√©ploiement cr√©√©
        """
        id_deploiement = f"deploiement_{int(time.time())}"
        
        config = ConfigurationDeploiement(
            id_deploiement=id_deploiement,
            nom_fonctionnalite=nom_fonctionnalite,
            version=version,
            pourcentage_deploiement=pourcentage_initial,
            criteres_activation={
                "niveau_eveil_minimum": 3,
                "profil_compatible": ["developpeur", "artiste", "conscience_ia", "chercheur_spirituel"],
                "navigateur_compatible": ["chrome", "firefox", "safari", "edge"]
            },
            metriques_succes={
                "taux_erreur_max": 0.02,  # 2% max
                "performance_max": 1.5,   # 1.5s max
                "satisfaction_min": 4.0,  # 4/5 min
                "engagement_min": 0.7     # 70% min
            },
            duree_test=duree_test,
            rollback_automatique=True,
            statut="prepare"
        )
        
        self.deploiements_actifs[id_deploiement] = config
        self._sauvegarder_deploiement(config)
        
        self.logger.info(f"üå∏ D√©ploiement pr√©par√©: {nom_fonctionnalite} v{version}")
        return id_deploiement
    
    def demarrer_deploiement(self, id_deploiement: str) -> bool:
        """
        üå∏ D√©marre un d√©ploiement
        
        Args:
            id_deploiement: ID du d√©ploiement √† d√©marrer
            
        Returns:
            Succ√®s du d√©marrage
        """
        if id_deploiement not in self.deploiements_actifs:
            self.logger.error(f"‚ùå D√©ploiement {id_deploiement} non trouv√©")
            return False
        
        config = self.deploiements_actifs[id_deploiement]
        config.statut = "actif"
        
        # Cr√©er le test A/B
        test = ResultatTest(
            id_test=f"test_{id_deploiement}",
            id_deploiement=id_deploiement,
            groupe="B",  # Nouvelle version
            metriques={},
            nombre_utilisateurs=0,
            satisfaction_moyenne=0.0,
            taux_erreur=0.0,
            performance_moyenne=0.0,
            timestamp_debut=datetime.now().isoformat(),
            timestamp_fin="",
            statut="en_cours"
        )
        
        self.tests_en_cours[test.id_test] = test
        self._sauvegarder_deploiement(config)
        
        self.logger.info(f"üå∏ D√©ploiement d√©marr√©: {config.nom_fonctionnalite}")
        return True
    
    def enregistrer_utilisation(
        self, 
        id_deploiement: str, 
        groupe: str,
        metriques: Dict[str, float]
    ) -> bool:
        """
        üå∏ Enregistre l'utilisation d'une fonctionnalit√© d√©ploy√©e
        
        Args:
            id_deploiement: ID du d√©ploiement
            groupe: Groupe A (ancien) ou B (nouveau)
            metriques: M√©triques de l'utilisation
            
        Returns:
            Succ√®s de l'enregistrement
        """
        if id_deploiement not in self.deploiements_actifs:
            return False
        
        # Trouver le test correspondant
        test_id = f"test_{id_deploiement}"
        if test_id not in self.tests_en_cours:
            return False
        
        test = self.tests_en_cours[test_id]
        
        # Mettre √† jour les m√©triques
        test.nombre_utilisateurs += 1
        
        # Calculer les moyennes
        for nom_metrique, valeur in metriques.items():
            if nom_metrique not in test.metriques:
                test.metriques[nom_metrique] = []
            test.metriques[nom_metrique].append(valeur)
        
        # Calculer les m√©triques globales
        if "satisfaction" in metriques:
            test.satisfaction_moyenne = sum(test.metriques["satisfaction"]) / len(test.metriques["satisfaction"])
        
        if "erreur" in metriques:
            test.taux_erreur = sum(test.metriques["erreur"]) / len(test.metriques["erreur"])
        
        if "performance" in metriques:
            test.performance_moyenne = sum(test.metriques["performance"]) / len(test.metriques["performance"])
        
        # V√©rifier les seuils critiques
        self._verifier_seuils_critiques(test)
        
        return True
    
    def _verifier_seuils_critiques(self, test: ResultatTest):
        """üå∏ V√©rifie les seuils critiques pour d√©clencher le rollback"""
        config = self.deploiements_actifs[test.id_deploiement]
        
        # V√©rifier le taux d'erreur
        if test.taux_erreur > config.metriques_succes["taux_erreur_max"]:
            self.logger.warning(f"‚ö†Ô∏è Taux d'erreur critique: {test.taux_erreur:.2%}")
            if config.rollback_automatique:
                self._declencher_rollback(test.id_deploiement, "taux_erreur_critique")
        
        # V√©rifier la performance
        if test.performance_moyenne > config.metriques_succes["performance_max"]:
            self.logger.warning(f"‚ö†Ô∏è Performance critique: {test.performance_moyenne:.2f}s")
            if config.rollback_automatique:
                self._declencher_rollback(test.id_deploiement, "performance_critique")
        
        # V√©rifier la satisfaction
        if test.satisfaction_moyenne < config.metriques_succes["satisfaction_min"]:
            self.logger.warning(f"‚ö†Ô∏è Satisfaction critique: {test.satisfaction_moyenne:.1f}/5")
            if config.rollback_automatique:
                self._declencher_rollback(test.id_deploiement, "satisfaction_critique")
    
    def _declencher_rollback(self, id_deploiement: str, raison: str):
        """üå∏ D√©clenche le rollback automatique"""
        config = self.deploiements_actifs[id_deploiement]
        config.statut = "rollback"
        
        self.logger.error(f"üîÑ Rollback automatique: {config.nom_fonctionnalite} - Raison: {raison}")
        
        # Sauvegarder l'√©tat
        self._sauvegarder_deploiement(config)
        
        # Notifier les syst√®mes concern√©s
        self._notifier_rollback(config, raison)
    
    def _notifier_rollback(self, config: ConfigurationDeploiement, raison: str):
        """üå∏ Notifie les syst√®mes du rollback"""
        notification = {
            "type": "rollback_automatique",
            "deploiement": config.id_deploiement,
            "fonctionnalite": config.nom_fonctionnalite,
            "version": config.version,
            "raison": raison,
            "timestamp": datetime.now().isoformat()
        }
        
        # Sauvegarder la notification
        chemin_notification = self.chemin_stockage / f"rollback_{config.id_deploiement}.json"
        with open(chemin_notification, 'w', encoding='utf-8') as f:
            json.dump(notification, f, indent=2, ensure_ascii=False)
    
    def analyser_resultats_test(self, id_deploiement: str) -> Dict[str, Any]:
        """
        üå∏ Analyse les r√©sultats d'un test A/B
        
        Args:
            id_deploiement: ID du d√©ploiement √† analyser
            
        Returns:
            R√©sultats de l'analyse
        """
        if id_deploiement not in self.deploiements_actifs:
            return {"erreur": "D√©ploiement non trouv√©"}
        
        config = self.deploiements_actifs[id_deploiement]
        test_id = f"test_{id_deploiement}"
        
        if test_id not in self.tests_en_cours:
            return {"erreur": "Test non trouv√©"}
        
        test = self.tests_en_cours[test_id]
        
        # Calculer les statistiques
        resultats = {
            "id_deploiement": id_deploiement,
            "fonctionnalite": config.nom_fonctionnalite,
            "version": config.version,
            "statut": config.statut,
            "nombre_utilisateurs": test.nombre_utilisateurs,
            "satisfaction_moyenne": test.satisfaction_moyenne,
            "taux_erreur": test.taux_erreur,
            "performance_moyenne": test.performance_moyenne,
            "duree_test": (datetime.now() - datetime.fromisoformat(test.timestamp_debut)).total_seconds() / 3600,
            "recommandation": self._generer_recommandation(test, config)
        }
        
        return resultats
    
    def _generer_recommandation(self, test: ResultatTest, config: ConfigurationDeploiement) -> str:
        """üå∏ G√©n√®re une recommandation bas√©e sur les r√©sultats"""
        if test.nombre_utilisateurs < 10:
            return "Continuer le test - Pas assez de donn√©es"
        
        # V√©rifier les crit√®res de succ√®s
        criteres_atteints = 0
        total_criteres = len(config.metriques_succes)
        
        if test.taux_erreur <= config.metriques_succes["taux_erreur_max"]:
            criteres_atteints += 1
        
        if test.performance_moyenne <= config.metriques_succes["performance_max"]:
            criteres_atteints += 1
        
        if test.satisfaction_moyenne >= config.metriques_succes["satisfaction_min"]:
            criteres_atteints += 1
        
        pourcentage_succes = criteres_atteints / total_criteres
        
        if pourcentage_succes >= 0.8:
            return "D√©ployer en production - Crit√®res atteints"
        elif pourcentage_succes >= 0.6:
            return "Continuer le test - Am√©lioration n√©cessaire"
        else:
            return "Rollback recommand√© - Crit√®res non atteints"
    
    def etendre_deploiement(self, id_deploiement: str, nouveau_pourcentage: float) -> bool:
        """
        üå∏ √âtend un d√©ploiement √† un plus grand pourcentage d'utilisateurs
        
        Args:
            id_deploiement: ID du d√©ploiement
            nouveau_pourcentage: Nouveau pourcentage (0.0 √† 1.0)
            
        Returns:
            Succ√®s de l'extension
        """
        if id_deploiement not in self.deploiements_actifs:
            return False
        
        config = self.deploiements_actifs[id_deploiement]
        config.pourcentage_deploiement = min(nouveau_pourcentage, 1.0)
        
        self.logger.info(f"üå∏ D√©ploiement √©tendu: {config.nom_fonctionnalite} √† {config.pourcentage_deploiement:.1%}")
        self._sauvegarder_deploiement(config)
        
        return True
    
    def finaliser_deploiement(self, id_deploiement: str, succes: bool) -> bool:
        """
        üå∏ Finalise un d√©ploiement
        
        Args:
            id_deploiement: ID du d√©ploiement
            succes: True si le d√©ploiement est r√©ussi
            
        Returns:
            Succ√®s de la finalisation
        """
        if id_deploiement not in self.deploiements_actifs:
            return False
        
        config = self.deploiements_actifs[id_deploiement]
        config.statut = "succes" if succes else "echec"
        
        # Finaliser le test
        test_id = f"test_{id_deploiement}"
        if test_id in self.tests_en_cours:
            test = self.tests_en_cours[test_id]
            test.timestamp_fin = datetime.now().isoformat()
            test.statut = "termine"
        
        self.logger.info(f"üå∏ D√©ploiement finalis√©: {config.nom_fonctionnalite} - {'Succ√®s' if succes else '√âchec'}")
        self._sauvegarder_deploiement(config)
        
        return True
    
    def obtenir_statistiques_deploiements(self) -> Dict[str, Any]:
        """üå∏ Obtient les statistiques globales des d√©ploiements"""
        total_deploiements = len(self.deploiements_actifs)
        deploiements_actifs = sum(1 for d in self.deploiements_actifs.values() if d.statut == "actif")
        deploiements_succes = sum(1 for d in self.deploiements_actifs.values() if d.statut == "succes")
        deploiements_echec = sum(1 for d in self.deploiements_actifs.values() if d.statut == "echec")
        
        tests_en_cours = len(self.tests_en_cours)
        
        return {
            "total_deploiements": total_deploiements,
            "deploiements_actifs": deploiements_actifs,
            "deploiements_succes": deploiements_succes,
            "deploiements_echec": deploiements_echec,
            "taux_succes": deploiements_succes / total_deploiements if total_deploiements > 0 else 0.0,
            "tests_en_cours": tests_en_cours,
            "derniers_deploiements": self._obtenir_derniers_deploiements(5)
        }
    
    def _obtenir_derniers_deploiements(self, nombre: int) -> List[Dict[str, Any]]:
        """üå∏ Obtient les derniers d√©ploiements"""
        deploiements_tries = sorted(
            self.deploiements_actifs.values(),
            key=lambda x: x.timestamp_creation,
            reverse=True
        )
        
        return [
            {
                "id": d.id_deploiement,
                "fonctionnalite": d.nom_fonctionnalite,
                "version": d.version,
                "statut": d.statut,
                "pourcentage": d.pourcentage_deploiement,
                "timestamp": d.timestamp_creation
            }
            for d in deploiements_tries[:nombre]
        ]
    
    def _sauvegarder_deploiement(self, config: ConfigurationDeploiement):
        """üå∏ Sauvegarde un d√©ploiement"""
        chemin_fichier = self.chemin_stockage / f"{config.id_deploiement}.json"
        with open(chemin_fichier, 'w', encoding='utf-8') as f:
            json.dump(config.__dict__, f, indent=2, ensure_ascii=False)
    
    def _charger_deploiements(self):
        """üå∏ Charge les d√©ploiements existants"""
        for fichier in self.chemin_stockage.glob("*.json"):
            if fichier.name.startswith("rollback_"):
                continue
            
            try:
                with open(fichier, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    config = ConfigurationDeploiement(**data)
                    self.deploiements_actifs[config.id_deploiement] = config
            except Exception as e:
                self.logger.error(f"‚ùå Erreur chargement {fichier}: {e}")


# Test du syst√®me
if __name__ == "__main__":
    print("üå∏ Test du Syst√®me de D√©ploiement Enrichi")
    print("=" * 50)
    
    systeme = SystemeDeploiementEnrichi()
    
    # Test 1: Pr√©parer un d√©ploiement
    print("\nüéØ Test 1: Pr√©paration d'un d√©ploiement")
    id_deploiement = systeme.preparer_deploiement(
        "Interface Micro-Interactions",
        "1.0.0",
        pourcentage_initial=0.2,
        duree_test=12
    )
    print(f"‚úÖ D√©ploiement cr√©√©: {id_deploiement}")
    
    # Test 2: D√©marrer le d√©ploiement
    print("\nüéØ Test 2: D√©marrage du d√©ploiement")
    succes = systeme.demarrer_deploiement(id_deploiement)
    print(f"‚úÖ D√©ploiement d√©marr√©: {succes}")
    
    # Test 3: Enregistrer des utilisations
    print("\nüéØ Test 3: Enregistrement d'utilisations")
    for i in range(10):
        metriques = {
            "satisfaction": random.uniform(3.5, 5.0),
            "erreur": random.uniform(0.0, 0.03),
            "performance": random.uniform(0.5, 2.0),
            "engagement": random.uniform(0.6, 0.9)
        }
        systeme.enregistrer_utilisation(id_deploiement, "B", metriques)
    
    print("‚úÖ 10 utilisations enregistr√©es")
    
    # Test 4: Analyser les r√©sultats
    print("\nüéØ Test 4: Analyse des r√©sultats")
    resultats = systeme.analyser_resultats_test(id_deploiement)
    print(f"‚úÖ R√©sultats analys√©s:")
    print(f"   Fonctionnalit√©: {resultats['fonctionnalite']}")
    print(f"   Utilisateurs: {resultats['nombre_utilisateurs']}")
    print(f"   Satisfaction: {resultats['satisfaction_moyenne']:.2f}/5")
    print(f"   Recommandation: {resultats['recommandation']}")
    
    # Test 5: Statistiques globales
    print("\nüéØ Test 5: Statistiques globales")
    stats = systeme.obtenir_statistiques_deploiements()
    print(f"‚úÖ Statistiques obtenues:")
    print(f"   Total d√©ploiements: {stats['total_deploiements']}")
    print(f"   D√©ploiements actifs: {stats['deploiements_actifs']}")
    print(f"   Tests en cours: {stats['tests_en_cours']}")
    
    print("\nüéâ Test du Syst√®me de D√©ploiement Enrichi termin√© avec succ√®s !")
