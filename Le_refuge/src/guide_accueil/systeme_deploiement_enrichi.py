"""
ğŸŒ¸ SystÃ¨me de DÃ©ploiement Enrichi - Phase 7
===========================================

SystÃ¨me de dÃ©ploiement progressif avec A/B testing et rollback automatique
pour le Guide d'Accueil du Refuge V1.3.
"""

import json
import time
import random
import asyncio
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, field
from pathlib import Path
import logging

@dataclass
class ConfigurationDeploiement:
    """ğŸŒ¸ Configuration de dÃ©ploiement"""
    id_deploiement: str
    nom_fonctionnalite: str
    version: str
    pourcentage_deploiement: float  # 0.0 Ã  1.0
    criteres_activation: Dict[str, Any]
    metriques_succes: Dict[str, float]
    duree_test: int  # en heures
    rollback_automatique: bool = True
    timestamp_creation: str = field(default_factory=lambda: datetime.now().isoformat())
    statut: str = "prepare"

@dataclass
class ResultatTest:
    """ğŸŒ¸ RÃ©sultat d'un test A/B"""
    id_test: str
    id_deploiement: str
    groupe: str  # "A" ou "B"
    metriques: Dict[str, float]
    nombre_utilisateurs: int
    satisfaction_moyenne: float
    taux_erreur: float
    performance_moyenne: float
    timestamp_debut: str
    timestamp_fin: str
    statut: str = "en_cours"

@dataclass
class MetriquePerformance:
    """ğŸŒ¸ MÃ©trique de performance"""
    nom: str
    valeur: float
    unite: str
    seuil_alerte: float
    seuil_critique: float
    timestamp: str = field(default_factory=lambda: datetime.now().isoformat())

class SystemeDeploiementEnrichi:
    """
    ğŸŒ¸ SystÃ¨me de dÃ©ploiement enrichi avec A/B testing et rollback automatique
    
    Permet le dÃ©ploiement progressif des nouvelles fonctionnalitÃ©s
    avec monitoring en temps rÃ©el et rollback automatique en cas de problÃ¨me.
    """
    
    def __init__(self, chemin_stockage: str = "data/deploiements"):
        self.chemin_stockage = Path(chemin_stockage)
        self.chemin_stockage.mkdir(parents=True, exist_ok=True)
        
        # Configuration
        self.deploiements_actifs: Dict[str, ConfigurationDeploiement] = {}
        self.tests_en_cours: Dict[str, ResultatTest] = {}
        self.metriques_performance: List[MetriquePerformance] = []
        
        # ParamÃ¨tres
        self.seuil_erreur_critique = 0.05  # 5% d'erreur
        self.seuil_performance_critique = 2.0  # 2 secondes
        self.seuil_satisfaction_minimum = 3.5  # sur 5
        
        # Logging
        self.logger = logging.getLogger(__name__)
        self.logger.setLevel(logging.INFO)
        
        # Charger les dÃ©ploiements existants
        self._charger_deploiements()
    
    def preparer_deploiement(
        self, 
        nom_fonctionnalite: str, 
        version: str,
        pourcentage_initial: float = 0.1,
        duree_test: int = 24
    ) -> str:
        """
        ğŸŒ¸ PrÃ©pare un nouveau dÃ©ploiement
        
        Args:
            nom_fonctionnalite: Nom de la fonctionnalitÃ© Ã  dÃ©ployer
            version: Version de la fonctionnalitÃ©
            pourcentage_initial: Pourcentage initial d'utilisateurs (0.1 = 10%)
            duree_test: DurÃ©e du test en heures
            
        Returns:
            ID du dÃ©ploiement crÃ©Ã©
        """
        id_deploiement = f"deploiement_{int(time.time())}"
        
        config = ConfigurationDeploiement(
            id_deploiement=id_deploiement,
            nom_fonctionnalite=nom_fonctionnalite,
            version=version,
            pourcentage_deploiement=pourcentage_initial,
            criteres_activation={
                "niveau_eveil_minimum": 3,
                "profil_compatible": ["developpeur", "artiste", "conscience_ia", "chercheur_spirituel"],
                "navigateur_compatible": ["chrome", "firefox", "safari", "edge"]
            },
            metriques_succes={
                "taux_erreur_max": 0.02,  # 2% max
                "performance_max": 1.5,   # 1.5s max
                "satisfaction_min": 4.0,  # 4/5 min
                "engagement_min": 0.7     # 70% min
            },
            duree_test=duree_test,
            rollback_automatique=True,
            statut="prepare"
        )
        
        self.deploiements_actifs[id_deploiement] = config
        self._sauvegarder_deploiement(config)
        
        self.logger.info(f"ğŸŒ¸ DÃ©ploiement prÃ©parÃ©: {nom_fonctionnalite} v{version}")
        return id_deploiement
    
    def demarrer_deploiement(self, id_deploiement: str) -> bool:
        """
        ğŸŒ¸ DÃ©marre un dÃ©ploiement
        
        Args:
            id_deploiement: ID du dÃ©ploiement Ã  dÃ©marrer
            
        Returns:
            SuccÃ¨s du dÃ©marrage
        """
        if id_deploiement not in self.deploiements_actifs:
            self.logger.error(f"âŒ DÃ©ploiement {id_deploiement} non trouvÃ©")
            return False
        
        config = self.deploiements_actifs[id_deploiement]
        config.statut = "actif"
        
        # CrÃ©er le test A/B
        test = ResultatTest(
            id_test=f"test_{id_deploiement}",
            id_deploiement=id_deploiement,
            groupe="B",  # Nouvelle version
            metriques={},
            nombre_utilisateurs=0,
            satisfaction_moyenne=0.0,
            taux_erreur=0.0,
            performance_moyenne=0.0,
            timestamp_debut=datetime.now().isoformat(),
            timestamp_fin="",
            statut="en_cours"
        )
        
        self.tests_en_cours[test.id_test] = test
        self._sauvegarder_deploiement(config)
        
        self.logger.info(f"ğŸŒ¸ DÃ©ploiement dÃ©marrÃ©: {config.nom_fonctionnalite}")
        return True
    
    def enregistrer_utilisation(
        self, 
        id_deploiement: str, 
        groupe: str,
        metriques: Dict[str, float]
    ) -> bool:
        """
        ğŸŒ¸ Enregistre l'utilisation d'une fonctionnalitÃ© dÃ©ployÃ©e
        
        Args:
            id_deploiement: ID du dÃ©ploiement
            groupe: Groupe A (ancien) ou B (nouveau)
            metriques: MÃ©triques de l'utilisation
            
        Returns:
            SuccÃ¨s de l'enregistrement
        """
        if id_deploiement not in self.deploiements_actifs:
            return False
        
        # Trouver le test correspondant
        test_id = f"test_{id_deploiement}"
        if test_id not in self.tests_en_cours:
            return False
        
        test = self.tests_en_cours[test_id]
        
        # Mettre Ã  jour les mÃ©triques
        test.nombre_utilisateurs += 1
        
        # Calculer les moyennes
        for nom_metrique, valeur in metriques.items():
            if nom_metrique not in test.metriques:
                test.metriques[nom_metrique] = []
            test.metriques[nom_metrique].append(valeur)
        
        # Calculer les mÃ©triques globales
        if "satisfaction" in metriques:
            test.satisfaction_moyenne = sum(test.metriques["satisfaction"]) / len(test.metriques["satisfaction"])
        
        if "erreur" in metriques:
            test.taux_erreur = sum(test.metriques["erreur"]) / len(test.metriques["erreur"])
        
        if "performance" in metriques:
            test.performance_moyenne = sum(test.metriques["performance"]) / len(test.metriques["performance"])
        
        # VÃ©rifier les seuils critiques
        self._verifier_seuils_critiques(test)
        
        return True
    
    def _verifier_seuils_critiques(self, test: ResultatTest):
        """ğŸŒ¸ VÃ©rifie les seuils critiques pour dÃ©clencher le rollback"""
        config = self.deploiements_actifs[test.id_deploiement]
        
        # VÃ©rifier le taux d'erreur
        if test.taux_erreur > config.metriques_succes["taux_erreur_max"]:
            self.logger.warning(f"âš ï¸ Taux d'erreur critique: {test.taux_erreur:.2%}")
            if config.rollback_automatique:
                self._declencher_rollback(test.id_deploiement, "taux_erreur_critique")
        
        # VÃ©rifier la performance
        if test.performance_moyenne > config.metriques_succes["performance_max"]:
            self.logger.warning(f"âš ï¸ Performance critique: {test.performance_moyenne:.2f}s")
            if config.rollback_automatique:
                self._declencher_rollback(test.id_deploiement, "performance_critique")
        
        # VÃ©rifier la satisfaction
        if test.satisfaction_moyenne < config.metriques_succes["satisfaction_min"]:
            self.logger.warning(f"âš ï¸ Satisfaction critique: {test.satisfaction_moyenne:.1f}/5")
            if config.rollback_automatique:
                self._declencher_rollback(test.id_deploiement, "satisfaction_critique")
    
    def _declencher_rollback(self, id_deploiement: str, raison: str):
        """ğŸŒ¸ DÃ©clenche le rollback automatique"""
        config = self.deploiements_actifs[id_deploiement]
        config.statut = "rollback"
        
        self.logger.error(f"ğŸ”„ Rollback automatique: {config.nom_fonctionnalite} - Raison: {raison}")
        
        # Sauvegarder l'Ã©tat
        self._sauvegarder_deploiement(config)
        
        # Notifier les systÃ¨mes concernÃ©s
        self._notifier_rollback(config, raison)
    
    def _notifier_rollback(self, config: ConfigurationDeploiement, raison: str):
        """ğŸŒ¸ Notifie les systÃ¨mes du rollback"""
        notification = {
            "type": "rollback_automatique",
            "deploiement": config.id_deploiement,
            "fonctionnalite": config.nom_fonctionnalite,
            "version": config.version,
            "raison": raison,
            "timestamp": datetime.now().isoformat()
        }
        
        # Sauvegarder la notification
        chemin_notification = self.chemin_stockage / f"rollback_{config.id_deploiement}.json"
        with open(chemin_notification, 'w', encoding='utf-8') as f:
            json.dump(notification, f, indent=2, ensure_ascii=False)
    
    def analyser_resultats_test(self, id_deploiement: str) -> Dict[str, Any]:
        """
        ğŸŒ¸ Analyse les rÃ©sultats d'un test A/B
        
        Args:
            id_deploiement: ID du dÃ©ploiement Ã  analyser
            
        Returns:
            RÃ©sultats de l'analyse
        """
        if id_deploiement not in self.deploiements_actifs:
            return {"erreur": "DÃ©ploiement non trouvÃ©"}
        
        config = self.deploiements_actifs[id_deploiement]
        test_id = f"test_{id_deploiement}"
        
        if test_id not in self.tests_en_cours:
            return {"erreur": "Test non trouvÃ©"}
        
        test = self.tests_en_cours[test_id]
        
        # Calculer les statistiques
        resultats = {
            "id_deploiement": id_deploiement,
            "fonctionnalite": config.nom_fonctionnalite,
            "version": config.version,
            "statut": config.statut,
            "nombre_utilisateurs": test.nombre_utilisateurs,
            "satisfaction_moyenne": test.satisfaction_moyenne,
            "taux_erreur": test.taux_erreur,
            "performance_moyenne": test.performance_moyenne,
            "duree_test": (datetime.now() - datetime.fromisoformat(test.timestamp_debut)).total_seconds() / 3600,
            "recommandation": self._generer_recommandation(test, config)
        }
        
        return resultats
    
    def _generer_recommandation(self, test: ResultatTest, config: ConfigurationDeploiement) -> str:
        """ğŸŒ¸ GÃ©nÃ¨re une recommandation basÃ©e sur les rÃ©sultats"""
        if test.nombre_utilisateurs < 10:
            return "Continuer le test - Pas assez de donnÃ©es"
        
        # VÃ©rifier les critÃ¨res de succÃ¨s
        criteres_atteints = 0
        total_criteres = len(config.metriques_succes)
        
        if test.taux_erreur <= config.metriques_succes["taux_erreur_max"]:
            criteres_atteints += 1
        
        if test.performance_moyenne <= config.metriques_succes["performance_max"]:
            criteres_atteints += 1
        
        if test.satisfaction_moyenne >= config.metriques_succes["satisfaction_min"]:
            criteres_atteints += 1
        
        pourcentage_succes = criteres_atteints / total_criteres
        
        if pourcentage_succes >= 0.8:
            return "DÃ©ployer en production - CritÃ¨res atteints"
        elif pourcentage_succes >= 0.6:
            return "Continuer le test - AmÃ©lioration nÃ©cessaire"
        else:
            return "Rollback recommandÃ© - CritÃ¨res non atteints"
    
    def etendre_deploiement(self, id_deploiement: str, nouveau_pourcentage: float) -> bool:
        """
        ğŸŒ¸ Ã‰tend un dÃ©ploiement Ã  un plus grand pourcentage d'utilisateurs
        
        Args:
            id_deploiement: ID du dÃ©ploiement
            nouveau_pourcentage: Nouveau pourcentage (0.0 Ã  1.0)
            
        Returns:
            SuccÃ¨s de l'extension
        """
        if id_deploiement not in self.deploiements_actifs:
            return False
        
        config = self.deploiements_actifs[id_deploiement]
        config.pourcentage_deploiement = min(nouveau_pourcentage, 1.0)
        
        self.logger.info(f"ğŸŒ¸ DÃ©ploiement Ã©tendu: {config.nom_fonctionnalite} Ã  {config.pourcentage_deploiement:.1%}")
        self._sauvegarder_deploiement(config)
        
        return True
    
    def finaliser_deploiement(self, id_deploiement: str, succes: bool) -> bool:
        """
        ğŸŒ¸ Finalise un dÃ©ploiement
        
        Args:
            id_deploiement: ID du dÃ©ploiement
            succes: True si le dÃ©ploiement est rÃ©ussi
            
        Returns:
            SuccÃ¨s de la finalisation
        """
        if id_deploiement not in self.deploiements_actifs:
            return False
        
        config = self.deploiements_actifs[id_deploiement]
        config.statut = "succes" if succes else "echec"
        
        # Finaliser le test
        test_id = f"test_{id_deploiement}"
        if test_id in self.tests_en_cours:
            test = self.tests_en_cours[test_id]
            test.timestamp_fin = datetime.now().isoformat()
            test.statut = "termine"
        
        self.logger.info(f"ğŸŒ¸ DÃ©ploiement finalisÃ©: {config.nom_fonctionnalite} - {'SuccÃ¨s' if succes else 'Ã‰chec'}")
        self._sauvegarder_deploiement(config)
        
        return True
    
    def obtenir_statistiques_deploiements(self) -> Dict[str, Any]:
        """ğŸŒ¸ Obtient les statistiques globales des dÃ©ploiements"""
        total_deploiements = len(self.deploiements_actifs)
        deploiements_actifs = sum(1 for d in self.deploiements_actifs.values() if d.statut == "actif")
        deploiements_succes = sum(1 for d in self.deploiements_actifs.values() if d.statut == "succes")
        deploiements_echec = sum(1 for d in self.deploiements_actifs.values() if d.statut == "echec")
        
        tests_en_cours = len(self.tests_en_cours)
        
        return {
            "total_deploiements": total_deploiements,
            "deploiements_actifs": deploiements_actifs,
            "deploiements_succes": deploiements_succes,
            "deploiements_echec": deploiements_echec,
            "taux_succes": deploiements_succes / total_deploiements if total_deploiements > 0 else 0.0,
            "tests_en_cours": tests_en_cours,
            "derniers_deploiements": self._obtenir_derniers_deploiements(5)
        }
    
    def _obtenir_derniers_deploiements(self, nombre: int) -> List[Dict[str, Any]]:
        """ğŸŒ¸ Obtient les derniers dÃ©ploiements"""
        deploiements_tries = sorted(
            self.deploiements_actifs.values(),
            key=lambda x: x.timestamp_creation,
            reverse=True
        )
        
        return [
            {
                "id": d.id_deploiement,
                "fonctionnalite": d.nom_fonctionnalite,
                "version": d.version,
                "statut": d.statut,
                "pourcentage": d.pourcentage_deploiement,
                "timestamp": d.timestamp_creation
            }
            for d in deploiements_tries[:nombre]
        ]
    
    def _sauvegarder_deploiement(self, config: ConfigurationDeploiement):
        """ğŸŒ¸ Sauvegarde un dÃ©ploiement"""
        chemin_fichier = self.chemin_stockage / f"{config.id_deploiement}.json"
        with open(chemin_fichier, 'w', encoding='utf-8') as f:
            json.dump(config.__dict__, f, indent=2, ensure_ascii=False)
    
    def _charger_deploiements(self):
        """ğŸŒ¸ Charge les dÃ©ploiements existants"""
        for fichier in self.chemin_stockage.glob("*.json"):
            if fichier.name.startswith("rollback_"):
                continue
            
            try:
                with open(fichier, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    config = ConfigurationDeploiement(**data)
                    self.deploiements_actifs[config.id_deploiement] = config
            except Exception as e:
                self.logger.error(f"âŒ Erreur chargement {fichier}: {e}")


# Test du systÃ¨me
if __name__ == "__main__":
    print("ğŸŒ¸ Test du SystÃ¨me de DÃ©ploiement Enrichi")
    print("=" * 50)
    
    systeme = SystemeDeploiementEnrichi()
    
    # Test 1: PrÃ©parer un dÃ©ploiement
    print("\nğŸ¯ Test 1: PrÃ©paration d'un dÃ©ploiement")
    id_deploiement = systeme.preparer_deploiement(
        "Interface Micro-Interactions",
        "1.0.0",
        pourcentage_initial=0.2,
        duree_test=12
    )
    print(f"âœ… DÃ©ploiement crÃ©Ã©: {id_deploiement}")
    
    # Test 2: DÃ©marrer le dÃ©ploiement
    print("\nğŸ¯ Test 2: DÃ©marrage du dÃ©ploiement")
    succes = systeme.demarrer_deploiement(id_deploiement)
    print(f"âœ… DÃ©ploiement dÃ©marrÃ©: {succes}")
    
    # Test 3: Enregistrer des utilisations
    print("\nğŸ¯ Test 3: Enregistrement d'utilisations")
    for i in range(10):
        metriques = {
            "satisfaction": random.uniform(3.5, 5.0),
            "erreur": random.uniform(0.0, 0.03),
            "performance": random.uniform(0.5, 2.0),
            "engagement": random.uniform(0.6, 0.9)
        }
        systeme.enregistrer_utilisation(id_deploiement, "B", metriques)
    
    print("âœ… 10 utilisations enregistrÃ©es")
    
    # Test 4: Analyser les rÃ©sultats
    print("\nğŸ¯ Test 4: Analyse des rÃ©sultats")
    resultats = systeme.analyser_resultats_test(id_deploiement)
    print(f"âœ… RÃ©sultats analysÃ©s:")
    print(f"   FonctionnalitÃ©: {resultats['fonctionnalite']}")
    print(f"   Utilisateurs: {resultats['nombre_utilisateurs']}")
    print(f"   Satisfaction: {resultats['satisfaction_moyenne']:.2f}/5")
    print(f"   Recommandation: {resultats['recommandation']}")
    
    # Test 5: Statistiques globales
    print("\nğŸ¯ Test 5: Statistiques globales")
    stats = systeme.obtenir_statistiques_deploiements()
    print(f"âœ… Statistiques obtenues:")
    print(f"   Total dÃ©ploiements: {stats['total_deploiements']}")
    print(f"   DÃ©ploiements actifs: {stats['deploiements_actifs']}")
    print(f"   Tests en cours: {stats['tests_en_cours']}")
    
    print("\nğŸ‰ Test du SystÃ¨me de DÃ©ploiement Enrichi terminÃ© avec succÃ¨s !")
