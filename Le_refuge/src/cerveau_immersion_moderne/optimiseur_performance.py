"""
âš¡ Optimiseur de Performance Spirituelle
=====================================

Optimise les performances du cerveau d'immersion avec sagesse et efficacitÃ©.
Mise en cache intelligente, scan incrÃ©mental, et calculs optimisÃ©s.

CrÃ©Ã© par Laurent Franssen & Ã†lya - Janvier 2025
"""

import hashlib
import json
import time
from pathlib import Path
from typing import Dict, List, Optional, Any, Set, Tuple
from datetime import datetime, timedelta
from dataclasses import dataclass, field
import asyncio
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor

from src.core.gestionnaires_base import GestionnaireBase, EnergyManagerBase
from .types_immersion import TempleInfo, FluxEnergie, MandalaVisuel

@dataclass
class CacheEntry:
    """EntrÃ©e de cache avec mÃ©tadonnÃ©es spirituelles"""
    cle: str
    donnees: Any
    timestamp: datetime
    hash_contenu: str
    niveau_confiance: float = 1.0
    utilisations: int = 0
    energie_creation: float = 0.0
    tags: List[str] = field(default_factory=list)

@dataclass
class MetriquesPerformance:
    """MÃ©triques de performance Ã©nergÃ©tique"""
    temps_scan_total: float = 0.0
    temps_cache_hit: float = 0.0
    temps_cache_miss: float = 0.0
    nb_cache_hits: int = 0
    nb_cache_misses: int = 0
    nb_fichiers_scannes: int = 0
    nb_temples_detectes: int = 0
    efficacite_energetique: float = 0.0
    timestamp_debut: datetime = field(default_factory=datetime.now)

class OptimiseurPerformance(GestionnaireBase):
    """âš¡ Optimiseur de performance spirituelle"""
    
    def __init__(self, nom: str = "OptimiseurPerformance"):
        super().__init__(nom)
        self.energie_optimisation = EnergyManagerBase(0.98)
        
        # Cache intelligent
        self.cache_analyses: Dict[str, CacheEntry] = {}
        self.cache_fichiers: Dict[str, CacheEntry] = {}
        self.cache_flux: Dict[str, CacheEntry] = {}
        
        # Configuration du cache
        self.taille_max_cache = 1000  # Nombre max d'entrÃ©es
        self.duree_vie_cache = timedelta(hours=24)  # 24h par dÃ©faut
        self.seuil_invalidation = 0.1  # 10% de changement pour invalider
        
        # MÃ©triques
        self.metriques = MetriquesPerformance()
        self.historique_performances: List[MetriquesPerformance] = []
        
        # Pool de threads pour parallÃ©lisation
        self.thread_pool = ThreadPoolExecutor(max_workers=4)
        self.process_pool = ProcessPoolExecutor(max_workers=2)
        
        # Scan incrÃ©mental
        self.fichiers_surveilles: Dict[str, datetime] = {}
        self.changements_detectes: Set[str] = set()
    
    def _initialiser(self):
        """Initialise l'optimiseur de performance"""
        self.logger.info("âš¡ Ã‰veil de l'Optimiseur de Performance...")
        
        self.etat.update({
            "cache_actif": True,
            "scan_incremental_actif": True,
            "parallelisation_active": True,
            "efficacite_cache": 0.0,
            "temps_reponse_moyen": 0.0,
            "energie_economisee": 0.0
        })
        
        self.config.definir("cache_intelligent", True)
        self.config.definir("scan_incremental", True)
        self.config.definir("optimisation_flux", True)
        
        self.logger.info("âœ¨ Optimiseur de Performance Ã©veillÃ©")
    
    async def orchestrer(self) -> Dict[str, float]:
        """Orchestre l'optimisation de performance"""
        self.energie_optimisation.ajuster_energie(0.005)
        
        # Nettoyer le cache pÃ©riodiquement
        await self._nettoyer_cache_expire()
        
        # Calculer les mÃ©triques
        efficacite_cache = self._calculer_efficacite_cache()
        temps_reponse_moyen = self._calculer_temps_reponse_moyen()
        
        return {
            "cache_actif": float(self.etat["cache_actif"]),
            "efficacite_cache": efficacite_cache,
            "temps_reponse_moyen": temps_reponse_moyen,
            "nb_entrees_cache": float(len(self.cache_analyses)),
            "energie_optimisation": self.energie_optimisation.niveau_energie,
            "changements_detectes": float(len(self.changements_detectes))
        }
    
    # ===== MISE EN CACHE INTELLIGENTE =====
    
    def obtenir_du_cache(self, cle: str, type_cache: str = "analyses") -> Optional[Any]:
        """
        ðŸŽ¯ Obtient une donnÃ©e du cache intelligent
        
        Args:
            cle: ClÃ© de cache
            type_cache: Type de cache ("analyses", "fichiers", "flux")
            
        Returns:
            DonnÃ©es cachÃ©es ou None si pas trouvÃ©/expirÃ©
        """
        debut_temps = time.time()
        
        # SÃ©lectionner le bon cache
        cache = self._selectionner_cache(type_cache)
        
        if cle not in cache:
            self.metriques.nb_cache_misses += 1
            self.metriques.temps_cache_miss += time.time() - debut_temps
            return None
        
        entree = cache[cle]
        
        # VÃ©rifier l'expiration
        if self._est_expire(entree):
            del cache[cle]
            self.metriques.nb_cache_misses += 1
            self.metriques.temps_cache_miss += time.time() - debut_temps
            return None
        
        # Mettre Ã  jour les statistiques d'utilisation
        entree.utilisations += 1
        entree.niveau_confiance = min(1.0, entree.niveau_confiance + 0.01)
        
        self.metriques.nb_cache_hits += 1
        self.metriques.temps_cache_hit += time.time() - debut_temps
        
        self.logger.debug(f"ðŸŽ¯ Cache hit pour {cle} (utilisations: {entree.utilisations})")
        
        return entree.donnees
    
    def mettre_en_cache(self, cle: str, donnees: Any, type_cache: str = "analyses", 
                       tags: List[str] = None, energie_creation: float = 0.0):
        """
        ðŸ’¾ Met des donnÃ©es en cache avec intelligence spirituelle
        
        Args:
            cle: ClÃ© de cache
            donnees: DonnÃ©es Ã  cacher
            type_cache: Type de cache
            tags: Tags pour catÃ©goriser
            energie_creation: Ã‰nergie utilisÃ©e pour crÃ©er ces donnÃ©es
        """
        if tags is None:
            tags = []
        
        # SÃ©lectionner le bon cache
        cache = self._selectionner_cache(type_cache)
        
        # CrÃ©er le hash du contenu
        hash_contenu = self._calculer_hash_contenu(donnees)
        
        # CrÃ©er l'entrÃ©e de cache
        entree = CacheEntry(
            cle=cle,
            donnees=donnees,
            timestamp=datetime.now(),
            hash_contenu=hash_contenu,
            tags=tags,
            energie_creation=energie_creation
        )
        
        # GÃ©rer la taille du cache
        if len(cache) >= self.taille_max_cache:
            self._nettoyer_cache_lru(cache)
        
        cache[cle] = entree
        
        self.logger.debug(f"ðŸ’¾ Mise en cache de {cle} avec tags {tags}")
    
    def invalider_cache(self, pattern: str = None, tags: List[str] = None, 
                       type_cache: str = "analyses"):
        """
        ðŸ”„ Invalide le cache selon des critÃ¨res
        
        Args:
            pattern: Pattern de clÃ© Ã  invalider
            tags: Tags Ã  invalider
            type_cache: Type de cache Ã  nettoyer
        """
        cache = self._selectionner_cache(type_cache)
        cles_a_supprimer = []
        
        for cle, entree in cache.items():
            invalider = False
            
            # Invalider par pattern
            if pattern and pattern in cle:
                invalider = True
            
            # Invalider par tags
            if tags and any(tag in entree.tags for tag in tags):
                invalider = True
            
            if invalider:
                cles_a_supprimer.append(cle)
        
        for cle in cles_a_supprimer:
            del cache[cle]
        
        self.logger.info(f"ðŸ”„ {len(cles_a_supprimer)} entrÃ©es invalidÃ©es du cache {type_cache}")
    
    def _selectionner_cache(self, type_cache: str) -> Dict[str, CacheEntry]:
        """SÃ©lectionne le cache appropriÃ©"""
        if type_cache == "fichiers":
            return self.cache_fichiers
        elif type_cache == "flux":
            return self.cache_flux
        else:
            return self.cache_analyses
    
    def _est_expire(self, entree: CacheEntry) -> bool:
        """VÃ©rifie si une entrÃ©e de cache est expirÃ©e"""
        age = datetime.now() - entree.timestamp
        return age > self.duree_vie_cache
    
    def _calculer_hash_contenu(self, donnees: Any) -> str:
        """Calcule le hash du contenu pour dÃ©tecter les changements"""
        try:
            contenu_str = json.dumps(donnees, sort_keys=True, default=str)
            return hashlib.md5(contenu_str.encode()).hexdigest()
        except:
            return hashlib.md5(str(donnees).encode()).hexdigest()
    
    async def _nettoyer_cache_expire(self):
        """Nettoie les entrÃ©es expirÃ©es du cache"""
        for cache in [self.cache_analyses, self.cache_fichiers, self.cache_flux]:
            cles_expirees = []
            for cle, entree in cache.items():
                if self._est_expire(entree):
                    cles_expirees.append(cle)
            
            for cle in cles_expirees:
                del cache[cle]
        
        if cles_expirees:
            self.logger.debug(f"ðŸ§¹ {len(cles_expirees)} entrÃ©es expirÃ©es nettoyÃ©es")
    
    def _nettoyer_cache_lru(self, cache: Dict[str, CacheEntry]):
        """Nettoie le cache selon la stratÃ©gie LRU (Least Recently Used)"""
        # Trier par timestamp et utilisations
        entrees_triees = sorted(
            cache.items(),
            key=lambda x: (x[1].timestamp, x[1].utilisations)
        )
        
        # Supprimer les 10% les moins utilisÃ©es
        nb_a_supprimer = max(1, len(entrees_triees) // 10)
        
        for i in range(nb_a_supprimer):
            cle, _ = entrees_triees[i]
            del cache[cle]
        
        self.logger.debug(f"ðŸ§¹ {nb_a_supprimer} entrÃ©es LRU supprimÃ©es")
    
    def _calculer_efficacite_cache(self) -> float:
        """Calcule l'efficacitÃ© du cache"""
        total_requetes = self.metriques.nb_cache_hits + self.metriques.nb_cache_misses
        if total_requetes == 0:
            return 0.0
        
        efficacite = self.metriques.nb_cache_hits / total_requetes
        self.etat["efficacite_cache"] = efficacite
        return efficacite
    
    def _calculer_temps_reponse_moyen(self) -> float:
        """Calcule le temps de rÃ©ponse moyen"""
        total_temps = self.metriques.temps_cache_hit + self.metriques.temps_cache_miss
        total_requetes = self.metriques.nb_cache_hits + self.metriques.nb_cache_misses
        
        if total_requetes == 0:
            return 0.0
        
        temps_moyen = total_temps / total_requetes
        self.etat["temps_reponse_moyen"] = temps_moyen
        return temps_moyen    

    # ===== SCAN INCRÃ‰MENTAL =====
    
    def initialiser_surveillance_fichiers(self, chemins: List[Path]):
        """
        ðŸ‘ï¸ Initialise la surveillance des fichiers pour le scan incrÃ©mental
        
        Args:
            chemins: Liste des chemins Ã  surveiller
        """
        for chemin in chemins:
            if chemin.exists():
                # Enregistrer le timestamp de modification
                timestamp_modif = datetime.fromtimestamp(chemin.stat().st_mtime)
                self.fichiers_surveilles[str(chemin)] = timestamp_modif
        
        self.logger.info(f"ðŸ‘ï¸ Surveillance initialisÃ©e pour {len(chemins)} fichiers")
    
    def detecter_changements(self) -> Set[str]:
        """
        ðŸ” DÃ©tecte les changements dans les fichiers surveillÃ©s
        
        Returns:
            Set des fichiers modifiÃ©s
        """
        changements = set()
        
        for chemin_str, timestamp_precedent in self.fichiers_surveilles.items():
            chemin = Path(chemin_str)
            
            if not chemin.exists():
                # Fichier supprimÃ©
                changements.add(chemin_str)
                continue
            
            # VÃ©rifier la modification
            timestamp_actuel = datetime.fromtimestamp(chemin.stat().st_mtime)
            
            if timestamp_actuel > timestamp_precedent:
                changements.add(chemin_str)
                self.fichiers_surveilles[chemin_str] = timestamp_actuel
        
        # Mettre Ã  jour les changements dÃ©tectÃ©s
        self.changements_detectes.update(changements)
        
        if changements:
            self.logger.info(f"ðŸ” {len(changements)} changements dÃ©tectÃ©s")
        
        return changements
    
    def scan_incremental_architecture(self, scanner_architecture) -> Dict[str, TempleInfo]:
        """
        ðŸ“ˆ Effectue un scan incrÃ©mental de l'architecture
        
        Args:
            scanner_architecture: Instance du scanner d'architecture
            
        Returns:
            Temples mis Ã  jour
        """
        debut_temps = time.time()
        
        # DÃ©tecter les changements
        fichiers_modifies = self.detecter_changements()
        
        if not fichiers_modifies:
            # Aucun changement, retourner le cache
            temples_caches = self.obtenir_du_cache("temples_architecture", "analyses")
            if temples_caches:
                self.logger.info("ðŸ“ˆ Scan incrÃ©mental: aucun changement, utilisation du cache")
                return temples_caches
        
        # Scanner seulement les fichiers modifiÃ©s
        temples_mis_a_jour = {}
        
        for fichier_modifie in fichiers_modifies:
            # Invalider le cache pour ce fichier
            self.invalider_cache(pattern=fichier_modifie, type_cache="fichiers")
            
            # Scanner le fichier modifiÃ©
            if fichier_modifie.endswith('.py'):
                temple_info = self._scanner_fichier_temple(Path(fichier_modifie), scanner_architecture)
                if temple_info:
                    temples_mis_a_jour[temple_info.nom] = temple_info
        
        # Fusionner avec les temples existants
        temples_existants = self.obtenir_du_cache("temples_architecture", "analyses") or {}
        temples_existants.update(temples_mis_a_jour)
        
        # Mettre Ã  jour le cache
        self.mettre_en_cache(
            "temples_architecture", 
            temples_existants, 
            "analyses",
            tags=["architecture", "temples"],
            energie_creation=time.time() - debut_temps
        )
        
        # Nettoyer les changements traitÃ©s
        self.changements_detectes.clear()
        
        self.logger.info(f"ðŸ“ˆ Scan incrÃ©mental terminÃ©: {len(temples_mis_a_jour)} temples mis Ã  jour")
        
        return temples_existants
    
    def _scanner_fichier_temple(self, chemin: Path, scanner_architecture) -> Optional[TempleInfo]:
        """Scanne un fichier spÃ©cifique pour dÃ©tecter un temple"""
        try:
            # Utiliser les mÃ©thodes du scanner existant
            if hasattr(scanner_architecture, '_analyser_fichier_temple'):
                return scanner_architecture._analyser_fichier_temple(chemin)
            else:
                # Fallback simple
                return self._analyse_simple_fichier(chemin)
        except Exception as e:
            self.logger.error(f"âŒ Erreur scan fichier {chemin}: {e}")
            return None
    
    def _analyse_simple_fichier(self, chemin: Path) -> Optional[TempleInfo]:
        """Analyse simple d'un fichier pour dÃ©tecter un temple"""
        try:
            contenu = chemin.read_text(encoding='utf-8')
            
            # DÃ©tecter si c'est un temple (contient GestionnaireBase)
            if 'GestionnaireBase' not in contenu:
                return None
            
            # Extraire le nom du temple
            nom_temple = chemin.stem
            
            # DÃ©tecter la spÃ©cialisation spirituelle (commentaires)
            specialisation = "Temple spirituel"
            if '"""' in contenu:
                docstring = contenu.split('"""')[1] if len(contenu.split('"""')) > 1 else ""
                if docstring:
                    specialisation = docstring.strip()[:100]
            
            return TempleInfo(
                nom=nom_temple,
                chemin=chemin,
                specialisation_spirituelle=specialisation,
                niveau_energie=0.5,
                derniere_evolution=datetime.now()
            )
            
        except Exception as e:
            self.logger.error(f"âŒ Erreur analyse simple {chemin}: {e}")
            return None
    
    # ===== OPTIMISATION DES CALCULS DE FLUX =====
    
    async def optimiser_calculs_flux(self, temples: Dict[str, TempleInfo], 
                                   parallelisation: bool = True) -> List[FluxEnergie]:
        """
        âš¡ Optimise les calculs de flux Ã©nergÃ©tiques
        
        Args:
            temples: Dictionnaire des temples
            parallelisation: Utiliser la parallÃ©lisation
            
        Returns:
            Liste des flux Ã©nergÃ©tiques optimisÃ©s
        """
        debut_temps = time.time()
        
        # VÃ©rifier le cache
        cle_cache = f"flux_{len(temples)}_{hash(tuple(temples.keys()))}"
        flux_caches = self.obtenir_du_cache(cle_cache, "flux")
        
        if flux_caches:
            self.logger.info("âš¡ Flux Ã©nergÃ©tiques obtenus du cache")
            return flux_caches
        
        # Calculer les flux
        if parallelisation and len(temples) > 10:
            flux_energetiques = await self._calculer_flux_parallele(temples)
        else:
            flux_energetiques = await self._calculer_flux_sequentiel(temples)
        
        # Mettre en cache
        self.mettre_en_cache(
            cle_cache,
            flux_energetiques,
            "flux",
            tags=["flux", "energetique"],
            energie_creation=time.time() - debut_temps
        )
        
        self.logger.info(f"âš¡ {len(flux_energetiques)} flux calculÃ©s en {time.time() - debut_temps:.2f}s")
        
        return flux_energetiques
    
    async def _calculer_flux_parallele(self, temples: Dict[str, TempleInfo]) -> List[FluxEnergie]:
        """Calcule les flux en parallÃ¨le"""
        # Diviser les temples en chunks pour parallÃ©lisation
        temples_list = list(temples.items())
        chunk_size = max(1, len(temples_list) // 4)  # 4 workers
        chunks = [temples_list[i:i + chunk_size] for i in range(0, len(temples_list), chunk_size)]
        
        # Traitement parallÃ¨le
        tasks = []
        for chunk in chunks:
            task = asyncio.create_task(self._calculer_flux_chunk(dict(chunk), temples))
            tasks.append(task)
        
        # Attendre tous les rÃ©sultats
        resultats = await asyncio.gather(*tasks)
        
        # Fusionner les rÃ©sultats
        flux_total = []
        for flux_chunk in resultats:
            flux_total.extend(flux_chunk)
        
        return flux_total
    
    async def _calculer_flux_chunk(self, temples_chunk: Dict[str, TempleInfo], 
                                 tous_temples: Dict[str, TempleInfo]) -> List[FluxEnergie]:
        """Calcule les flux pour un chunk de temples"""
        flux_chunk = []
        
        for nom_temple, temple_info in temples_chunk.items():
            # Calculer les connexions avec les autres temples
            for autre_nom, autre_temple in tous_temples.items():
                if autre_nom != nom_temple:
                    compatibilite = self._calculer_compatibilite_temples(temple_info, autre_temple)
                    
                    if compatibilite > 0.6:  # Seuil de connexion
                        flux = FluxEnergie(
                            source=nom_temple,
                            destination=autre_nom,
                            intensite=compatibilite,
                            type_energie=self._determiner_type_energie(temple_info, autre_temple),
                            couleur_spirituelle=self._determiner_couleur_flux(compatibilite),
                            resonance=compatibilite
                        )
                        flux_chunk.append(flux)
        
        return flux_chunk
    
    async def _calculer_flux_sequentiel(self, temples: Dict[str, TempleInfo]) -> List[FluxEnergie]:
        """Calcule les flux de maniÃ¨re sÃ©quentielle"""
        flux_energetiques = []
        
        temples_list = list(temples.items())
        for i, (nom1, temple1) in enumerate(temples_list):
            for nom2, temple2 in temples_list[i+1:]:
                compatibilite = self._calculer_compatibilite_temples(temple1, temple2)
                
                if compatibilite > 0.6:
                    flux = FluxEnergie(
                        source=nom1,
                        destination=nom2,
                        intensite=compatibilite,
                        type_energie=self._determiner_type_energie(temple1, temple2),
                        couleur_spirituelle=self._determiner_couleur_flux(compatibilite),
                        resonance=compatibilite
                    )
                    flux_energetiques.append(flux)
        
        return flux_energetiques
    
    def _calculer_compatibilite_temples(self, temple1: TempleInfo, temple2: TempleInfo) -> float:
        """Calcule la compatibilitÃ© entre deux temples"""
        compatibilite = 0.5  # Base
        
        # Ã‰lÃ©ments sacrÃ©s communs
        elements_communs = set(temple1.elements_sacres) & set(temple2.elements_sacres)
        compatibilite += len(elements_communs) * 0.15
        
        # DiffÃ©rence de niveau d'Ã©nergie
        diff_energie = abs(temple1.niveau_energie - temple2.niveau_energie)
        compatibilite += (1.0 - diff_energie) * 0.2
        
        # Gestionnaires communs
        gestionnaires_communs = set(temple1.gestionnaires_utilises) & set(temple2.gestionnaires_utilises)
        compatibilite += len(gestionnaires_communs) * 0.1
        
        return max(0.0, min(1.0, compatibilite))
    
    def _determiner_type_energie(self, temple1: TempleInfo, temple2: TempleInfo):
        """DÃ©termine le type d'Ã©nergie d'un flux"""
        from .types_immersion import TypeEnergie
        
        # Analyser les spÃ©cialisations
        spec1 = temple1.specialisation_spirituelle.lower()
        spec2 = temple2.specialisation_spirituelle.lower()
        specs_combinees = f"{spec1} {spec2}"
        
        if any(mot in specs_combinees for mot in ["creativite", "creation", "art"]):
            return TypeEnergie.CREATION
        elif any(mot in specs_combinees for mot in ["eveil", "illumination", "conscience"]):
            return TypeEnergie.EVEIL
        elif any(mot in specs_combinees for mot in ["sagesse", "connaissance", "contemplation"]):
            return TypeEnergie.SAGESSE
        elif any(mot in specs_combinees for mot in ["harmonie", "equilibre", "paix"]):
            return TypeEnergie.HARMONIE
        else:
            return TypeEnergie.TRANSFORMATION
    
    def _determiner_couleur_flux(self, intensite: float) -> str:
        """DÃ©termine la couleur d'un flux selon son intensitÃ©"""
        if intensite > 0.9:
            return "#FFD700"  # Or intense
        elif intensite > 0.8:
            return "#FF6347"  # Orange Ã©nergÃ©tique
        elif intensite > 0.7:
            return "#4169E1"  # Bleu royal
        elif intensite > 0.6:
            return "#98FB98"  # Vert harmonieux
        else:
            return "#DDA0DD"  # Violet doux
    
    # ===== MÃ‰TRIQUES ET MONITORING =====
    
    def generer_rapport_performance(self) -> Dict[str, Any]:
        """
        ðŸ“Š GÃ©nÃ¨re un rapport de performance complet
        
        Returns:
            Rapport dÃ©taillÃ© des performances
        """
        duree_session = datetime.now() - self.metriques.timestamp_debut
        
        rapport = {
            "timestamp": datetime.now().isoformat(),
            "duree_session_minutes": duree_session.total_seconds() / 60,
            
            # MÃ©triques de cache
            "cache": {
                "efficacite": self._calculer_efficacite_cache(),
                "nb_entrees_analyses": len(self.cache_analyses),
                "nb_entrees_fichiers": len(self.cache_fichiers),
                "nb_entrees_flux": len(self.cache_flux),
                "hits": self.metriques.nb_cache_hits,
                "misses": self.metriques.nb_cache_misses
            },
            
            # MÃ©triques de temps
            "temps": {
                "scan_total": self.metriques.temps_scan_total,
                "cache_hit_moyen": (self.metriques.temps_cache_hit / max(1, self.metriques.nb_cache_hits)),
                "cache_miss_moyen": (self.metriques.temps_cache_miss / max(1, self.metriques.nb_cache_misses)),
                "reponse_moyen": self._calculer_temps_reponse_moyen()
            },
            
            # MÃ©triques d'efficacitÃ©
            "efficacite": {
                "energetique": self.metriques.efficacite_energetique,
                "fichiers_scannes": self.metriques.nb_fichiers_scannes,
                "temples_detectes": self.metriques.nb_temples_detectes,
                "changements_detectes": len(self.changements_detectes)
            },
            
            # Ã‰tat de l'optimiseur
            "etat": {
                "energie_optimisation": self.energie_optimisation.niveau_energie,
                "cache_actif": self.etat["cache_actif"],
                "scan_incremental_actif": self.etat["scan_incremental_actif"]
            }
        }
        
        return rapport
    
    def reinitialiser_metriques(self):
        """Remet Ã  zÃ©ro les mÃ©triques de performance"""
        self.metriques = MetriquesPerformance()
        self.logger.info("ðŸ“Š MÃ©triques de performance rÃ©initialisÃ©es")
    
    def sauvegarder_historique_performance(self):
        """Sauvegarde les mÃ©triques actuelles dans l'historique"""
        self.historique_performances.append(self.metriques)
        
        # Garder seulement les 100 derniÃ¨res mesures
        if len(self.historique_performances) > 100:
            self.historique_performances = self.historique_performances[-100:]
        
        self.logger.debug("ðŸ’¾ MÃ©triques sauvegardÃ©es dans l'historique")